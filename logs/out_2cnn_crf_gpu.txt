
--------------Starting Epoch 0-------------------
Loaded dataset... 

----- Starting Epoch-0 Batch-0 ------
torch.float32
tensor(45613.3047, device='cuda:0', grad_fn=<AddBackward0>)
tensor(45236.4023, device='cuda:0', grad_fn=<AddBackward0>)
tensor(44713.9258, device='cuda:0', grad_fn=<AddBackward0>)
tensor(44141.9297, device='cuda:0', grad_fn=<AddBackward0>)
tensor(43526.8633, device='cuda:0', grad_fn=<AddBackward0>)
tensor(42871.0312, device='cuda:0', grad_fn=<AddBackward0>)
tensor(42182.4805, device='cuda:0', grad_fn=<AddBackward0>)
tensor(41470.8438, device='cuda:0', grad_fn=<AddBackward0>)
tensor(40741.6992, device='cuda:0', grad_fn=<AddBackward0>)
tensor(39992.0820, device='cuda:0', grad_fn=<AddBackward0>)
tensor(39121.2930, device='cuda:0', grad_fn=<AddBackward0>)
tensor(38411.7930, device='cuda:0', grad_fn=<AddBackward0>)
tensor(37763.2266, device='cuda:0', grad_fn=<AddBackward0>)
tensor(36976.9688, device='cuda:0', grad_fn=<AddBackward0>)
tensor(36315.8828, device='cuda:0', grad_fn=<AddBackward0>)
tensor(35697.6055, device='cuda:0', grad_fn=<AddBackward0>)
tensor(35173.2422, device='cuda:0', grad_fn=<AddBackward0>)
tensor(34676.9180, device='cuda:0', grad_fn=<AddBackward0>)
tensor(34214.7344, device='cuda:0', grad_fn=<AddBackward0>)
tensor(33799.5938, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-0 Step-0 TIME ELAPSED = 593.6000096797943
Params after OPT step  params tensor([-1.8160e-03, -3.9215e-03, -4.4905e-03,  ..., -9.5868e-05,
        -1.0105e-04,  1.7904e-04], device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[1.0006e+00, 9.1030e-04, 1.0023e+00, 1.0016e+00, 3.8406e-04],
        [1.0009e+00, 1.6122e-03, 1.0032e+00, 1.0024e+00, 1.0372e-03],
        [9.4851e-04, 1.0021e+00, 1.0035e+00, 2.7844e-03, 1.0015e+00],
        [6.9223e-04, 1.0019e+00, 1.0029e+00, 1.0025e+00, 1.0015e+00],
        [1.0003e+00, 1.0013e+00, 1.8771e-03, 1.0018e+00, 1.0010e+00]],
       device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[1.0081, 0.0072, 0.0042],
        [1.0094, 0.0084, 0.0054],
        [1.0084, 0.0077, 1.0052]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625]
	letter accuracy =  [0.5727482678983834]
Test Accuracy : 
	word accuracy =  [0.03125]
	letter accuracy =  [0.5225933202357563]

 avg_word_acc_train = 0.0625
 avg_word_acc_test = 0.03125
 avg_letter_acc_train = 0.5727482678983834
 avg_letter_acc_test = 0.5225933202357563

Batch completed Epoch-0 Batch-0 Step-0 TIME ELAPSED = 669.268360376358

----- Starting Epoch-0 Batch-1 ------
torch.float32
tensor(34842.7734, device='cuda:0', grad_fn=<AddBackward0>)
tensor(34654.9492, device='cuda:0', grad_fn=<AddBackward0>)
tensor(34423.6836, device='cuda:0', grad_fn=<AddBackward0>)
tensor(34145.1250, device='cuda:0', grad_fn=<AddBackward0>)
tensor(33830.4453, device='cuda:0', grad_fn=<AddBackward0>)
tensor(33572.6680, device='cuda:0', grad_fn=<AddBackward0>)
tensor(33291.1523, device='cuda:0', grad_fn=<AddBackward0>)
tensor(32995.7891, device='cuda:0', grad_fn=<AddBackward0>)
tensor(32708.9707, device='cuda:0', grad_fn=<AddBackward0>)
tensor(32384.2070, device='cuda:0', grad_fn=<AddBackward0>)
tensor(32084.9355, device='cuda:0', grad_fn=<AddBackward0>)
tensor(31763.3223, device='cuda:0', grad_fn=<AddBackward0>)
tensor(31444.1543, device='cuda:0', grad_fn=<AddBackward0>)
tensor(31113.0078, device='cuda:0', grad_fn=<AddBackward0>)
tensor(30793.9766, device='cuda:0', grad_fn=<AddBackward0>)
tensor(30484.0977, device='cuda:0', grad_fn=<AddBackward0>)
tensor(30171.2227, device='cuda:0', grad_fn=<AddBackward0>)
tensor(29902.0684, device='cuda:0', grad_fn=<AddBackward0>)
tensor(29627.1836, device='cuda:0', grad_fn=<AddBackward0>)
tensor(29348.4551, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-1 Step-1 TIME ELAPSED = 610.4182465076447
Params after OPT step  params tensor([-0.0025, -0.0030, -0.0023,  ..., -0.0006, -0.0006,  0.0014],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0015e+00,  4.4962e-04,  1.0099e+00,  1.0066e+00, -9.8510e-05],
        [ 1.0013e+00,  2.2729e-03,  1.0124e+00,  1.0081e+00,  1.3266e-03],
        [ 6.6678e-04,  1.0044e+00,  1.0124e+00,  8.7327e-03,  1.0028e+00],
        [ 3.2873e-05,  1.0047e+00,  1.0090e+00,  1.0075e+00,  1.0027e+00],
        [ 9.9964e-01,  1.0032e+00,  4.2666e-03,  1.0048e+00,  1.0012e+00]],
       device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[1.0278, 0.0204, 0.0051],
        [1.0312, 0.0219, 0.0082],
        [1.0255, 0.0182, 1.0074]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709]

 avg_word_acc_train = 0.0546875
 avg_word_acc_test = 0.0546875
 avg_letter_acc_train = 0.6144332056791495
 avg_letter_acc_test = 0.5600894166571135

Batch completed Epoch-0 Batch-1 Step-1 TIME ELAPSED = 690.5380091667175

----- Starting Epoch-0 Batch-2 ------
torch.float32
tensor(32142.0527, device='cuda:0', grad_fn=<AddBackward0>)
tensor(32098.6934, device='cuda:0', grad_fn=<AddBackward0>)
tensor(31957.3164, device='cuda:0', grad_fn=<AddBackward0>)
tensor(31757.1738, device='cuda:0', grad_fn=<AddBackward0>)
tensor(31516.1602, device='cuda:0', grad_fn=<AddBackward0>)
tensor(31248.6523, device='cuda:0', grad_fn=<AddBackward0>)
tensor(30966.4902, device='cuda:0', grad_fn=<AddBackward0>)
tensor(30679.4629, device='cuda:0', grad_fn=<AddBackward0>)
tensor(30392.9199, device='cuda:0', grad_fn=<AddBackward0>)
tensor(30105.8789, device='cuda:0', grad_fn=<AddBackward0>)
tensor(29808.8633, device='cuda:0', grad_fn=<AddBackward0>)
tensor(29465.7227, device='cuda:0', grad_fn=<AddBackward0>)
tensor(29187.8652, device='cuda:0', grad_fn=<AddBackward0>)
tensor(28963.3359, device='cuda:0', grad_fn=<AddBackward0>)
tensor(28701.1914, device='cuda:0', grad_fn=<AddBackward0>)
tensor(28406.5840, device='cuda:0', grad_fn=<AddBackward0>)
tensor(28117.1582, device='cuda:0', grad_fn=<AddBackward0>)
tensor(27794.7383, device='cuda:0', grad_fn=<AddBackward0>)
tensor(27494.1855, device='cuda:0', grad_fn=<AddBackward0>)
tensor(27169.9902, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-2 Step-2 TIME ELAPSED = 621.3149015903473
Params after OPT step  params tensor([-0.0010,  0.0016,  0.0037,  ..., -0.0011, -0.0012,  0.0031],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 9.9888e-01, -5.7328e-03,  1.0135e+00,  1.0093e+00, -9.8873e-04],
        [ 9.9632e-01, -4.8065e-03,  1.0132e+00,  1.0077e+00, -2.6446e-03],
        [-5.5976e-03,  9.9770e-01,  1.0098e+00,  5.2636e-03,  9.9742e-01],
        [-5.2690e-03,  9.9910e-01,  1.0032e+00,  1.0020e+00,  9.9727e-01],
        [ 9.9704e-01,  9.9928e-01, -2.9609e-03,  9.9873e-01,  9.9695e-01]],
       device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 1.0220,  0.0085, -0.0114],
        [ 1.0202,  0.0037, -0.0125],
        [ 1.0099, -0.0026,  0.9867]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782]

 avg_word_acc_train = 0.09375
 avg_word_acc_test = 0.057291666666666664
 avg_letter_acc_train = 0.6307821116254878
 avg_letter_acc_test = 0.5741175821192351

Batch completed Epoch-0 Batch-2 Step-2 TIME ELAPSED = 697.8307678699493

----- Starting Epoch-0 Batch-3 ------
torch.float32
tensor(27931.7148, device='cuda:0', grad_fn=<AddBackward0>)
tensor(27834.5469, device='cuda:0', grad_fn=<AddBackward0>)
tensor(27689.1055, device='cuda:0', grad_fn=<AddBackward0>)
tensor(27502.1641, device='cuda:0', grad_fn=<AddBackward0>)
tensor(27296.7344, device='cuda:0', grad_fn=<AddBackward0>)
tensor(27076.4727, device='cuda:0', grad_fn=<AddBackward0>)
tensor(26839.6309, device='cuda:0', grad_fn=<AddBackward0>)
tensor(26582.5195, device='cuda:0', grad_fn=<AddBackward0>)
tensor(26300.3359, device='cuda:0', grad_fn=<AddBackward0>)
tensor(25986.6484, device='cuda:0', grad_fn=<AddBackward0>)
tensor(25622.9531, device='cuda:0', grad_fn=<AddBackward0>)
tensor(25184.6230, device='cuda:0', grad_fn=<AddBackward0>)
tensor(24739.3418, device='cuda:0', grad_fn=<AddBackward0>)
tensor(24265.5117, device='cuda:0', grad_fn=<AddBackward0>)
tensor(23703.4629, device='cuda:0', grad_fn=<AddBackward0>)
tensor(23008.5840, device='cuda:0', grad_fn=<AddBackward0>)
tensor(22039.7637, device='cuda:0', grad_fn=<AddBackward0>)
tensor(20890.0566, device='cuda:0', grad_fn=<AddBackward0>)
tensor(19271.3398, device='cuda:0', grad_fn=<AddBackward0>)
tensor(16946.0527, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-3 Step-3 TIME ELAPSED = 620.3208620548248
Params after OPT step  params tensor([-0.0070, -0.0124,  0.0034,  ..., -0.0066, -0.0079,  0.0208],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9777, -0.0540,  1.0357,  1.0281,  0.0019],
        [ 0.9599, -0.0548,  1.0170,  1.0074, -0.0257],
        [-0.0462,  0.9565,  0.9937, -0.0134,  0.9596],
        [-0.0369,  0.9683,  0.9689,  0.9668,  0.9514],
        [ 0.9828,  0.9835, -0.0430,  0.9541,  0.9547]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.9720, -0.0761, -0.1021],
        [ 0.9427, -0.1112, -0.1367],
        [ 0.9070, -0.1341,  0.8476]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187]

 avg_word_acc_train = 0.09765625
 avg_word_acc_test = 0.0546875
 avg_letter_acc_train = 0.6436808460141978
 avg_letter_acc_test = 0.5823349114802561

Batch completed Epoch-0 Batch-3 Step-3 TIME ELAPSED = 698.7907276153564

----- Starting Epoch-0 Batch-4 ------
torch.float32
tensor(25907.0410, device='cuda:0', grad_fn=<AddBackward0>)
tensor(25813.3730, device='cuda:0', grad_fn=<AddBackward0>)
tensor(25350.5371, device='cuda:0', grad_fn=<AddBackward0>)
tensor(24695.9082, device='cuda:0', grad_fn=<AddBackward0>)
tensor(23895.6758, device='cuda:0', grad_fn=<AddBackward0>)
tensor(23006.9258, device='cuda:0', grad_fn=<AddBackward0>)
tensor(22092.8203, device='cuda:0', grad_fn=<AddBackward0>)
tensor(21177.6387, device='cuda:0', grad_fn=<AddBackward0>)
tensor(20253.5938, device='cuda:0', grad_fn=<AddBackward0>)
tensor(19340.7324, device='cuda:0', grad_fn=<AddBackward0>)
tensor(18457.8145, device='cuda:0', grad_fn=<AddBackward0>)
tensor(17596.4766, device='cuda:0', grad_fn=<AddBackward0>)
tensor(16756.6953, device='cuda:0', grad_fn=<AddBackward0>)
tensor(15942.4668, device='cuda:0', grad_fn=<AddBackward0>)
tensor(15154.1836, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14391.3662, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13661.0459, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12972.4277, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12189.4854, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11644.4912, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-4 Step-4 TIME ELAPSED = 615.4895775318146
Params after OPT step  params tensor([-0.0137, -0.0232, -0.0113,  ..., -0.0090, -0.0105,  0.0303],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9420, -0.0964,  0.9183,  1.0262,  0.0456],
        [ 0.9044, -0.1244,  0.8813,  1.0038, -0.0134],
        [-0.1019,  0.8757,  0.8855, -0.0226,  0.9374],
        [-0.0835,  0.8962,  0.9133,  0.9401,  0.9035],
        [ 0.9477,  0.9370, -0.0497,  0.9111,  0.9049]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.6216, -0.1791, -0.0444],
        [ 0.5729, -0.2201, -0.1849],
        [ 0.5937, -0.2696,  0.7079]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636]

 avg_word_acc_train = 0.090625
 avg_word_acc_test = 0.0625
 avg_letter_acc_train = 0.648818916162271
 avg_letter_acc_test = 0.5885952019114776

Batch completed Epoch-0 Batch-4 Step-4 TIME ELAPSED = 694.8864440917969

----- Starting Epoch-0 Batch-5 ------
torch.float32
tensor(15128.5498, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14863.0127, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14501.7812, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14155.1797, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13829.4600, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13516.3867, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13224.7920, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12942.0420, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12682.3037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12433.3486, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12198.1094, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11966.9580, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11741.5303, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11514.3311, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11273.9854, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11058.6289, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10842.5332, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10653.2949, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10460.0020, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10242.1797, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-5 Step-5 TIME ELAPSED = 621.0539095401764
Params after OPT step  params tensor([-0.0107, -0.0417, -0.0260,  ..., -0.0139, -0.0141,  0.0540],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9091, -0.1355,  0.7696,  1.0702,  0.1023],
        [ 0.8578, -0.1808,  0.7773,  1.0839,  0.0018],
        [-0.1576,  0.8052,  0.8487,  0.0400,  0.9177],
        [-0.1319,  0.8378,  0.9378,  0.9555,  0.8575],
        [ 0.9186,  0.8919, -0.0048,  0.8879,  0.8523]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.3415, -0.1050,  0.0450],
        [ 0.3969, -0.1032, -0.2531],
        [ 0.4435, -0.2949,  0.4765]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619]

 avg_word_acc_train = 0.08854166666666667
 avg_word_acc_test = 0.0703125
 avg_letter_acc_train = 0.6483696402081218
 avg_letter_acc_test = 0.5974801285770249

Batch completed Epoch-0 Batch-5 Step-5 TIME ELAPSED = 700.3676228523254

----- Starting Epoch-0 Batch-6 ------
torch.float32
tensor(12435.3730, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12406.9434, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12293.3555, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12144.4521, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11976.1650, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11793.4600, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11599.4141, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11396.1504, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11185.5537, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10968.8594, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10746.7949, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10521.1143, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10275.4297, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10046.3838, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9843.2939, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9639.6064, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9426.3203, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9186.6533, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8975.5410, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8773.6299, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-6 Step-6 TIME ELAPSED = 621.2310564517975
Params after OPT step  params tensor([-0.0038, -0.0570, -0.0245,  ..., -0.0156, -0.0175,  0.0781],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9226, -0.1413,  0.7868,  1.0544,  0.1134],
        [ 0.8552, -0.1845,  0.7905,  1.0984,  0.0190],
        [-0.1731,  0.7896,  0.8334,  0.0497,  0.9437],
        [-0.1545,  0.8046,  0.8908,  0.9472,  0.8785],
        [ 0.9067,  0.8442, -0.0729,  0.8703,  0.8606]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.2947, -0.1898,  0.1055],
        [ 0.4451, -0.0559, -0.2347],
        [ 0.3890, -0.2448,  0.4620]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908]

 avg_word_acc_train = 0.09151785714285714
 avg_word_acc_test = 0.07142857142857142
 avg_letter_acc_train = 0.6568263290594886
 avg_letter_acc_test = 0.600506004152663

Batch completed Epoch-0 Batch-6 Step-6 TIME ELAPSED = 701.1639404296875

----- Starting Epoch-0 Batch-7 ------
torch.float32
tensor(14185.1650, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14121.6338, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14037.6104, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13920.2930, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13785.1133, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13639.6826, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13487.9814, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13331.3008, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13169.4238, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13001.4199, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12826.1025, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12642.6113, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12450.5850, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12250.4434, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12043.0059, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11829.7705, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11612.3389, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11392.6055, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11169.8154, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10942.2383, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-7 Step-7 TIME ELAPSED = 620.8227298259735
Params after OPT step  params tensor([-0.0012, -0.0683, -0.0236,  ..., -0.0168, -0.0209,  0.0984],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9265, -0.1801,  0.7803,  1.0820,  0.1093],
        [ 0.8361, -0.2376,  0.8030,  1.1264,  0.0225],
        [-0.2012,  0.7345,  0.8524,  0.0460,  0.9531],
        [-0.1723,  0.7558,  0.9046,  0.9112,  0.8842],
        [ 0.9017,  0.8024, -0.0685,  0.8160,  0.8625]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.2172, -0.1893,  0.1072],
        [ 0.4628, -0.0108, -0.2561],
        [ 0.3287, -0.2654,  0.4397]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697]

 avg_word_acc_train = 0.08984375
 avg_word_acc_test = 0.064453125
 avg_letter_acc_train = 0.6547711148501294
 avg_letter_acc_test = 0.5891264657547922

Batch completed Epoch-0 Batch-7 Step-7 TIME ELAPSED = 699.4657056331635

----- Starting Epoch-0 Batch-8 ------
torch.float32
tensor(10566.8613, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10543.9131, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10481.0195, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10398.6504, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10306.5439, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10209.1094, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10107.9355, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10003.3086, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9895.0645, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9782.8789, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9666.2725, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9544.6172, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9417.2969, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9283.9980, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9144.6865, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8999.0400, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8845.6807, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8681.0146, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8497.6777, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8285.3438, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-8 Step-8 TIME ELAPSED = 626.6700558662415
Params after OPT step  params tensor([-0.0016, -0.0727, -0.0307,  ..., -0.0172, -0.0208,  0.0999],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9036, -0.2533,  0.7532,  1.1518,  0.0948],
        [ 0.8118, -0.2844,  0.7913,  1.1628,  0.0131],
        [-0.2200,  0.7203,  0.8493,  0.0459,  0.9381],
        [-0.1660,  0.7882,  0.9138,  0.8938,  0.8678],
        [ 0.9102,  0.8658, -0.0558,  0.7898,  0.8411]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.2821, -0.1962,  0.0633],
        [ 0.4912, -0.0348, -0.1802],
        [ 0.3509, -0.2990,  0.5216]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821]

 avg_word_acc_train = 0.10416666666666667
 avg_word_acc_test = 0.07465277777777778
 avg_letter_acc_train = 0.6609475918705402
 avg_letter_acc_test = 0.5935723109873355

Batch completed Epoch-0 Batch-8 Step-8 TIME ELAPSED = 706.4289512634277

----- Starting Epoch-0 Batch-9 ------
torch.float32
tensor(11922.7529, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11872.4395, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11786.2090, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11645.0596, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11481.6055, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11311.6904, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11141.6416, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10972.6025, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10803.6270, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10633.0273, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10458.9434, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10279.6289, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10093.6797, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9900.2734, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9699.3037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9491.4834, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9278.0771, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9060.1201, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8836.3018, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8601.0420, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-9 Step-9 TIME ELAPSED = 616.8969511985779
Params after OPT step  params tensor([ 0.0045, -0.0684, -0.0152,  ..., -0.0178, -0.0234,  0.1268],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.8916, -0.2523,  0.7377,  1.1671,  0.1037],
        [ 0.7948, -0.2821,  0.8055,  1.2071,  0.0550],
        [-0.2396,  0.6904,  0.8607,  0.0807,  0.9817],
        [-0.1811,  0.7241,  0.9160,  0.8991,  0.8857],
        [ 0.8994,  0.7998, -0.0755,  0.7565,  0.8340]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.2832, -0.2581, -0.0247],
        [ 0.5311, -0.0035, -0.2475],
        [ 0.2998, -0.2652,  0.4727]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818]

 avg_word_acc_train = 0.1125
 avg_word_acc_test = 0.0921875
 avg_letter_acc_train = 0.6691962670269206
 avg_letter_acc_test = 0.6023968980704202

Batch completed Epoch-0 Batch-9 Step-9 TIME ELAPSED = 696.7398972511292

----- Starting Epoch-0 Batch-10 ------
torch.float32
tensor(11967.4873, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11930.6221, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11770.2402, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11561.3174, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11330.4678, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11092.2070, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10852.7168, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10602.2881, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10329.8906, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10034.0010, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9765.7305, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9500.6250, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9220.8975, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8935.0557, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8688.2627, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8445.9062, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8147.7607, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7893.5845, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7617.6582, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7400.0713, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-10 Step-10 TIME ELAPSED = 619.8134939670563
Params after OPT step  params tensor([-0.0020, -0.1040, -0.0433,  ..., -0.0227, -0.0291,  0.2000],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.8770, -0.2784,  0.6925,  1.3041,  0.1322],
        [ 0.8168, -0.3391,  0.8065,  1.3553,  0.0726],
        [-0.2291,  0.5994,  0.8808,  0.1363,  0.9324],
        [-0.1531,  0.6443,  0.9905,  0.8620,  0.7619],
        [ 0.9422,  0.7492,  0.0021,  0.6385,  0.7092]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.2256, -0.2355, -0.0412],
        [ 0.6312,  0.1305, -0.2705],
        [ 0.2102, -0.3023,  0.4644]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769]

 avg_word_acc_train = 0.13352272727272727
 avg_word_acc_test = 0.08948863636363637
 avg_letter_acc_train = 0.6776879738441829
 avg_letter_acc_test = 0.6000810961479344

Batch completed Epoch-0 Batch-10 Step-10 TIME ELAPSED = 696.3981311321259

----- Starting Epoch-0 Batch-11 ------
torch.float32
tensor(11749.6211, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11707.4971, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11473.6758, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11178.8271, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10846.4746, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10498.6055, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10156.1338, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9836.2109, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9545.8496, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9283.0439, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9041.8779, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8818.3428, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8610.8135, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8412.7559, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8211.4199, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7988.0757, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7769.5137, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7531.2749, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7310.4941, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7061.3008, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-11 Step-11 TIME ELAPSED = 617.7649598121643
Params after OPT step  params tensor([-0.0189, -0.0895, -0.0414,  ..., -0.0234, -0.0281,  0.1888],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9050, -0.3411,  0.7897,  1.2504,  0.0318],
        [ 0.7600, -0.4479,  0.8392,  1.1924, -0.0429],
        [-0.2915,  0.6179,  0.8481, -0.0167,  0.9204],
        [-0.1754,  0.7531,  0.9261,  0.8493,  0.8436],
        [ 0.9724,  0.8835, -0.0714,  0.7295,  0.8137]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.1773, -0.2351, -0.0156],
        [ 0.7154,  0.0643, -0.1970],
        [ 0.2075, -0.2912,  0.4877]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634]

 avg_word_acc_train = 0.13932291666666666
 avg_word_acc_test = 0.09114583333333333
 avg_letter_acc_train = 0.685370182439455
 avg_letter_acc_test = 0.6053548661884117

Batch completed Epoch-0 Batch-11 Step-11 TIME ELAPSED = 696.3093764781952

----- Starting Epoch-0 Batch-12 ------
torch.float32
tensor(11317.0264, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11267.1152, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11158.4238, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11009.0400, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10836.6973, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10652.3818, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10463.2803, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10273.9414, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10087.0059, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9903.4814, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9723.2627, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9545.3906, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9368.3330, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9190.2725, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9009.3418, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8823.9609, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8633.0254, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8435.2129, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8226.0557, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7986.1831, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-12 Step-12 TIME ELAPSED = 609.6815836429596
Params after OPT step  params tensor([-0.0199, -0.1052, -0.0464,  ..., -0.0251, -0.0306,  0.2112],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9596, -0.2597,  0.7454,  1.2236,  0.0403],
        [ 0.7719, -0.3888,  0.8431,  1.2013, -0.0253],
        [-0.3003,  0.6459,  0.8716,  0.0081,  0.9510],
        [-0.2215,  0.6993,  0.9450,  0.8675,  0.8474],
        [ 0.9476,  0.8285, -0.0628,  0.7101,  0.7964]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0737, -0.2961,  0.0635],
        [ 0.7668,  0.1482, -0.2194],
        [ 0.1426, -0.2533,  0.3463]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297]

 avg_word_acc_train = 0.14903846153846154
 avg_word_acc_test = 0.09134615384615384
 avg_letter_acc_train = 0.6901062280602819
 avg_letter_acc_test = 0.60722215568389

Batch completed Epoch-0 Batch-12 Step-12 TIME ELAPSED = 687.0045030117035

----- Starting Epoch-0 Batch-13 ------
torch.float32
tensor(10172.8506, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10140.7754, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9971.7988, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9740.5283, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9476.5537, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9208.6523, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8956.9814, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8732.9531, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8537.0781, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8365.9375, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8209.7549, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8059.1338, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7911.1924, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7763.4097, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7613.1992, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7459.0918, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7293.2178, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7122.2451, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6944.3735, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6777.5405, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-13 Step-13 TIME ELAPSED = 619.608891248703
Params after OPT step  params tensor([-0.0008, -0.0973, -0.0136,  ..., -0.0232, -0.0300,  0.2037],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9599, -0.2009,  0.7698,  1.1710,  0.1141],
        [ 0.8124, -0.2994,  0.8244,  1.2190,  0.0458],
        [-0.2366,  0.6770,  0.8442,  0.0579,  0.9811],
        [-0.1745,  0.6791,  0.8982,  0.8691,  0.8560],
        [ 0.9792,  0.7950, -0.1233,  0.7041,  0.8317]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.2328, -0.3506,  0.0896],
        [ 0.6801,  0.1509, -0.2434],
        [ 0.1808, -0.2009,  0.4388]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622]

 avg_word_acc_train = 0.15959821428571427
 avg_word_acc_test = 0.09263392857142858
 avg_letter_acc_train = 0.697688482931533
 avg_letter_acc_test = 0.6111464418609095

Batch completed Epoch-0 Batch-13 Step-13 TIME ELAPSED = 698.8615272045135

----- Starting Epoch-0 Batch-14 ------
torch.float32
tensor(10628.1475, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10600.6631, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10514.7734, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10394.5576, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10255.2363, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10106.3955, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9954.0664, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9801.9268, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9652.0166, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9505.0752, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9361.0703, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9219.4004, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9079.0459, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8938.5273, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8796.1709, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8649.8701, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8497.1084, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8334.9902, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8159.9873, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7967.6099, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-14 Step-14 TIME ELAPSED = 607.2408781051636
Params after OPT step  params tensor([ 2.9820e-05, -8.6266e-02, -1.3853e-02,  ..., -2.4043e-02,
        -3.1082e-02,  2.1642e-01], device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9503, -0.2383,  0.7002,  1.1770,  0.1520],
        [ 0.7729, -0.3208,  0.8140,  1.2338,  0.0900],
        [-0.2914,  0.6633,  0.8530,  0.0647,  1.0032],
        [-0.1986,  0.6701,  0.9261,  0.8805,  0.8483],
        [ 1.0049,  0.7812, -0.1071,  0.7157,  0.8126]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.1597, -0.3169,  0.0635],
        [ 0.6849,  0.1204, -0.2759],
        [ 0.1613, -0.2424,  0.4023]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489]

 avg_word_acc_train = 0.16666666666666666
 avg_word_acc_test = 0.09583333333333334
 avg_letter_acc_train = 0.7022778457774197
 avg_letter_acc_test = 0.6146210328116788

Batch completed Epoch-0 Batch-14 Step-14 TIME ELAPSED = 685.4584002494812

----- Starting Epoch-0 Batch-15 ------
torch.float32
tensor(8450.0391, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8427.1875, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8335.7354, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8216.1982, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8074.3457, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7914.0381, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7737.5771, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7547.4336, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7339.9854, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7122.8457, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6896.7852, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6689.0659, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6478.8931, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6255.1050, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6006.4424, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5808.2729, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5608.1743, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5355.4180, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5115.5254, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4859.9678, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-15 Step-15 TIME ELAPSED = 622.569985628128
Params after OPT step  params tensor([-0.0097, -0.0302,  0.0115,  ..., -0.0266, -0.0348,  0.2492],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9678, -0.3028,  0.7306,  1.1958,  0.1488],
        [ 0.7298, -0.3933,  0.8910,  1.2131,  0.0944],
        [-0.3808,  0.6305,  0.8707, -0.0084,  1.0418],
        [-0.2773,  0.6490,  0.9384,  0.8479,  0.8831],
        [ 1.0193,  0.7781, -0.1266,  0.6584,  0.7760]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.1765, -0.3272, -0.0059],
        [ 0.9292,  0.1405, -0.1465],
        [-0.0070, -0.3339,  0.5236]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066]

 avg_word_acc_train = 0.1796875
 avg_word_acc_test = 0.0966796875
 avg_letter_acc_train = 0.7106785452932884
 avg_letter_acc_test = 0.6131065317620931

Batch completed Epoch-0 Batch-15 Step-15 TIME ELAPSED = 700.5799026489258

----- Starting Epoch-0 Batch-16 ------
torch.float32
tensor(15170.5244, device='cuda:0', grad_fn=<AddBackward0>)
tensor(15102.7861, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14689.3457, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14137.0078, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13572.2617, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13027.2559, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12524.6445, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12079.8125, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11691.3584, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11342.4775, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11019.8135, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10718.4570, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10433.5264, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10158.3682, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9884.8613, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9601.7627, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9295.4805, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9042.1680, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8804.2393, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8544.9365, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-16 Step-16 TIME ELAPSED = 611.4447572231293
Params after OPT step  params tensor([ 0.0050, -0.0792, -0.0357,  ..., -0.0245, -0.0313,  0.2277],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9258, -0.2622,  0.7417,  1.2185,  0.1359],
        [ 0.7659, -0.3831,  0.8393,  1.2525,  0.0630],
        [-0.2817,  0.6163,  0.8750,  0.0453,  0.9828],
        [-0.1790,  0.6625,  0.9935,  0.8930,  0.8591],
        [ 1.0320,  0.7961, -0.0635,  0.6869,  0.7934]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.2006, -0.3814, -0.0271],
        [ 0.7765,  0.1539, -0.2246],
        [ 0.0855, -0.2979,  0.5087]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541]

 avg_word_acc_train = 0.18474264705882354
 avg_word_acc_test = 0.10018382352941177
 avg_letter_acc_train = 0.7140324602653605
 avg_letter_acc_test = 0.6186277483122496

Batch completed Epoch-0 Batch-16 Step-16 TIME ELAPSED = 690.4554905891418

----- Starting Epoch-0 Batch-17 ------
torch.float32
tensor(10303.0791, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10273.0537, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10168.2773, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10028.8232, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9867.2617, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9691.5283, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9507.0176, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9316.9219, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9122.3066, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8922.3877, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8714.9980, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8497.1387, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8265.2754, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8015.7280, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7744.0493, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7441.8315, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7065.2061, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6784.5889, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6492.3945, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6113.3569, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-17 Step-17 TIME ELAPSED = 616.537201166153
Params after OPT step  params tensor([ 0.0219, -0.0855, -0.0592,  ..., -0.0285, -0.0363,  0.2799],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.8381, -0.2707,  0.6968,  1.2305,  0.2236],
        [ 0.6225, -0.4370,  0.8179,  1.2492,  0.1278],
        [-0.4038,  0.6108,  0.8997, -0.0329,  1.0544],
        [-0.2370,  0.6501,  1.0972,  0.8811,  0.9091],
        [ 1.0522,  0.7502, -0.0109,  0.6820,  0.8176]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0808, -0.4078,  0.0371],
        [ 0.9173,  0.3188, -0.3197],
        [-0.0673, -0.3152,  0.4794]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708]

 avg_word_acc_train = 0.19010416666666666
 avg_word_acc_test = 0.1032986111111111
 avg_letter_acc_train = 0.7195809863878597
 avg_letter_acc_test = 0.6209636599228675

Batch completed Epoch-0 Batch-17 Step-17 TIME ELAPSED = 693.7689080238342

----- Starting Epoch-0 Batch-18 ------
torch.float32
tensor(13386.3447, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13336.6016, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13056.2695, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12690.1660, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12305.2832, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11932.6934, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11583.8027, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11260.2285, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10958.6904, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10674.1914, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10402.1729, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10139.5088, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9883.9893, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9633.3447, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9384.5391, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9134.2793, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8878.0732, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8609.1006, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8283.2188, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8038.1411, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-18 Step-18 TIME ELAPSED = 613.5201201438904
Params after OPT step  params tensor([ 0.0097, -0.0427,  0.0053,  ..., -0.0257, -0.0341,  0.2437],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9451, -0.2461,  0.7398,  1.2050,  0.2176],
        [ 0.7248, -0.3966,  0.8137,  1.2363,  0.1098],
        [-0.3265,  0.6242,  0.7987, -0.0253,  1.0184],
        [-0.1931,  0.6541,  0.9663,  0.8966,  0.8350],
        [ 1.0802,  0.8147, -0.0635,  0.7337,  0.8205]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.1575, -0.4253,  0.0431],
        [ 0.8204,  0.1709, -0.2953],
        [ 0.0703, -0.2971,  0.4542]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384]

 avg_word_acc_train = 0.19407894736842105
 avg_word_acc_test = 0.10361842105263158
 avg_letter_acc_train = 0.7214321371943422
 avg_letter_acc_test = 0.6242770814601502

Batch completed Epoch-0 Batch-18 Step-18 TIME ELAPSED = 692.819230556488

----- Starting Epoch-0 Batch-19 ------
torch.float32
tensor(11300.9092, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11268.0742, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11151.3887, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10991.7402, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10803.1914, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10593.7422, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10368.6309, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10131.6719, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9886.3730, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9635.7109, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9381.8037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9124.7520, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8862.8652, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8575.7285, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8313.4736, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8060.5005, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7755.3105, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7413.3037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7124.9526, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6810.7646, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-19 Step-19 TIME ELAPSED = 616.2889988422394
Params after OPT step  params tensor([ 1.9416e-03, -8.2059e-02,  9.6810e-05,  ..., -2.7483e-02,
        -4.0346e-02,  3.0275e-01], device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9727, -0.2138,  0.7814,  1.2612,  0.2461],
        [ 0.7628, -0.4444,  0.8576,  1.3663,  0.0939],
        [-0.2306,  0.5631,  0.7352, -0.0280,  0.9933],
        [-0.1587,  0.4932,  0.9315,  0.9618,  0.6865],
        [ 1.1491,  0.7750, -0.1024,  0.7510,  0.7456]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0869, -0.4796,  0.0590],
        [ 0.9066,  0.3546, -0.4002],
        [-0.0906, -0.1706,  0.4644]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398]

 avg_word_acc_train = 0.196875
 avg_word_acc_test = 0.10390625
 avg_letter_acc_train = 0.724547038271133
 avg_letter_acc_test = 0.6223105392150996

Batch completed Epoch-0 Batch-19 Step-19 TIME ELAPSED = 693.4900321960449

----- Starting Epoch-0 Batch-20 ------
torch.float32
tensor(13735.7744, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13674.8311, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13461.2373, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13142.8555, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12782.2676, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12405.8525, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12029.6992, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11666.6475, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11328.1172, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11023.1025, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10749.1855, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10488.8887, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10223.6543, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9945.7412, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9653.3965, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9341.8252, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8976.9863, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8639.8701, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8320.3789, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7962.5522, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-20 Step-20 TIME ELAPSED = 626.0868413448334
Params after OPT step  params tensor([-6.5972e-05, -1.2079e-01, -3.2530e-02,  ..., -2.7937e-02,
        -4.0980e-02,  3.1189e-01], device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9291, -0.2655,  0.7526,  1.2623,  0.1800],
        [ 0.7068, -0.4764,  0.8566,  1.3337,  0.0633],
        [-0.3283,  0.5645,  0.7739, -0.0475,  0.9648],
        [-0.2333,  0.5518,  0.9648,  0.9667,  0.7955],
        [ 1.1285,  0.8403, -0.1087,  0.7165,  0.7435]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0738, -0.4843,  0.0537],
        [ 0.9462,  0.2776, -0.2654],
        [-0.0848, -0.3763,  0.5296]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119]

 avg_word_acc_train = 0.20014880952380953
 avg_word_acc_test = 0.10491071428571429
 avg_letter_acc_train = 0.7267849453140479
 avg_letter_acc_test = 0.6218139034486098

Batch completed Epoch-0 Batch-20 Step-20 TIME ELAPSED = 705.4716508388519

----- Starting Epoch-0 Batch-21 ------
torch.float32
tensor(11458.8447, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11416.8613, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11237.8574, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10995.0957, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10708.0576, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10396.1855, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10069.3711, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9730.4834, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9385.5186, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9096.3916, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8826.9150, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8549.8037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8248.0166, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7920.0322, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7584.3213, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7246.4341, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6907.1060, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6528.3706, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6179.9897, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5806.3081, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-21 Step-21 TIME ELAPSED = 623.517386674881
Params after OPT step  params tensor([ 0.0111, -0.1312, -0.0841,  ..., -0.0296, -0.0441,  0.3270],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9457, -0.2784,  0.6677,  1.2762,  0.2064],
        [ 0.7109, -0.4736,  0.8455,  1.3978,  0.0739],
        [-0.3501,  0.5397,  0.7660,  0.0068,  0.9397],
        [-0.2209,  0.5030,  1.0213,  1.0843,  0.7768],
        [ 1.1227,  0.8310, -0.1549,  0.6945,  0.7444]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 2.2650e-02, -5.0663e-01,  3.1809e-05],
        [ 1.0327e+00,  3.7248e-01, -3.5054e-01],
        [-6.8570e-02, -3.4742e-01,  4.4519e-01]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165]

 avg_word_acc_train = 0.20667613636363635
 avg_word_acc_test = 0.10511363636363637
 avg_letter_acc_train = 0.7325033405922394
 avg_letter_acc_test = 0.62222226118526

Batch completed Epoch-0 Batch-21 Step-21 TIME ELAPSED = 703.0402946472168

----- Starting Epoch-0 Batch-22 ------
torch.float32
tensor(11554.1396, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11499.2900, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11319.5967, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11055.2041, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10734.3477, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10380.5557, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10023.0430, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9685.7783, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9379.9658, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9105.4609, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8855.2998, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8619.6377, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8388.4150, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8152.4106, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7903.5254, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7634.9614, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7341.3794, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7018.3848, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6661.3892, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6250.8940, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-22 Step-22 TIME ELAPSED = 609.2362625598907
Params after OPT step  params tensor([ 0.0062, -0.1154, -0.0611,  ..., -0.0311, -0.0459,  0.3452],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0309, -0.2704,  0.7067,  1.2489,  0.2345],
        [ 0.7451, -0.4785,  0.8882,  1.3618,  0.0576],
        [-0.3419,  0.5299,  0.7594, -0.0214,  0.9399],
        [-0.2064,  0.5401,  0.9445,  1.0095,  0.7750],
        [ 1.1679,  0.8666, -0.2011,  0.6970,  0.7871]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0406, -0.4570, -0.0342],
        [ 1.0560,  0.3470, -0.2848],
        [-0.1334, -0.4187,  0.5555]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209]

 avg_word_acc_train = 0.21195652173913043
 avg_word_acc_test = 0.10529891304347826
 avg_letter_acc_train = 0.7354564400643205
 avg_letter_acc_test = 0.6255954267087758

Batch completed Epoch-0 Batch-22 Step-22 TIME ELAPSED = 686.372302532196

----- Starting Epoch-0 Batch-23 ------
torch.float32
tensor(10568.3369, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10523.6631, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10269.8994, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9965.0723, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9647.6143, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9328.0908, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9006.0850, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8684.0234, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8365.2861, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8052.5073, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7746.0967, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7434.1763, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7119.3716, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6840.2539, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6578.7832, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6298.8271, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5999.5220, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5689.5757, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5402.0469, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5108.8867, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-23 Step-23 TIME ELAPSED = 609.7082185745239
Params after OPT step  params tensor([ 0.0456, -0.1081, -0.0919,  ..., -0.0305, -0.0497,  0.3593],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0013, -0.3117,  0.6879,  1.2034,  0.2104],
        [ 0.7689, -0.5276,  0.8128,  1.3367,  0.0988],
        [-0.2849,  0.5121,  0.8009, -0.0048,  1.0377],
        [-0.1706,  0.5435,  1.0035,  0.9329,  0.7773],
        [ 1.1519,  0.8843, -0.0817,  0.6702,  0.8315]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0085, -0.4512,  0.0389],
        [ 0.9949,  0.4086, -0.3864],
        [-0.0977, -0.3113,  0.5069]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127]

 avg_word_acc_train = 0.22005208333333334
 avg_word_acc_test = 0.10872395833333333
 avg_letter_acc_train = 0.7392888229082483
 avg_letter_acc_test = 0.6232167488235857

Batch completed Epoch-0 Batch-23 Step-23 TIME ELAPSED = 686.5610730648041

----- Starting Epoch-0 Batch-24 ------
torch.float32
tensor(10791.4023, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10751.0859, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10537.2764, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10259.1973, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9960.8408, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9668.0645, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9389.3428, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9121.8379, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8860.4609, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8601.4482, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8341.9619, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8079.2729, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7809.9761, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7529.7383, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7233.5498, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6916.8281, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6570.5620, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6164.0288, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5770.1685, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5466.0439, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-24 Step-24 TIME ELAPSED = 605.0182967185974
Params after OPT step  params tensor([ 0.0396, -0.0399, -0.0908,  ..., -0.0311, -0.0502,  0.3511],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9527, -0.2575,  0.7510,  1.1597,  0.2043],
        [ 0.6411, -0.4954,  0.8486,  1.2383,  0.0643],
        [-0.3931,  0.5728,  0.8341, -0.0710,  1.0376],
        [-0.2345,  0.6004,  1.0536,  0.9778,  0.8173],
        [ 1.1808,  0.9043, -0.0812,  0.7070,  0.8553]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0242, -0.4719,  0.1988],
        [ 1.1087,  0.3632, -0.3864],
        [-0.1299, -0.3889,  0.4754]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246]

 avg_word_acc_train = 0.22375
 avg_word_acc_test = 0.105
 avg_letter_acc_train = 0.7431585655384769
 avg_letter_acc_test = 0.6227644649076032

Batch completed Epoch-0 Batch-24 Step-24 TIME ELAPSED = 681.7816481590271

----- Starting Epoch-0 Batch-25 ------
torch.float32
tensor(11709.7246, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11647.1045, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11429.2070, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11123.6016, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10782.7881, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10434.6367, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10094.2217, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9767.3662, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9455.3037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9154.9375, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8862.9248, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8575.0010, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8286.4961, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7991.2920, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7682.6987, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7342.7905, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6973.9062, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6631.8262, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6291.5259, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5918.4214, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-25 Step-25 TIME ELAPSED = 613.7679877281189
Params after OPT step  params tensor([ 0.0283, -0.0788, -0.1383,  ..., -0.0337, -0.0508,  0.3706],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9567, -0.2732,  0.6625,  1.1925,  0.2258],
        [ 0.6356, -0.4976,  0.8280,  1.3105,  0.1235],
        [-0.4122,  0.5415,  0.8196, -0.0420,  1.0593],
        [-0.2316,  0.4959,  1.0774,  1.0419,  0.8580],
        [ 1.2157,  0.8426, -0.0950,  0.7141,  0.8331]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0207, -0.6267,  0.1714],
        [ 1.1167,  0.3993, -0.4018],
        [-0.0782, -0.3262,  0.3875]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646]

 avg_word_acc_train = 0.22716346153846154
 avg_word_acc_test = 0.10757211538461539
 avg_letter_acc_train = 0.745784800631847
 avg_letter_acc_test = 0.6236415862670479

Batch completed Epoch-0 Batch-25 Step-25 TIME ELAPSED = 691.942519903183

----- Starting Epoch-0 Batch-26 ------
torch.float32
tensor(10866.7393, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10819.9434, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10578.6211, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10281.9248, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9961.4570, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9634.1328, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9309.6650, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8991.1641, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8680.0967, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8375.5312, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8066.4297, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7759.0518, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7468.9731, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7199.4927, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6931.3018, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6634.3291, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6327.4414, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6049.9165, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5775.8110, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5542.3047, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-26 Step-26 TIME ELAPSED = 620.7222054004669
Params after OPT step  params tensor([ 0.0297, -0.0390, -0.0131,  ..., -0.0383, -0.0527,  0.4162],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9760, -0.2144,  0.7301,  1.2276,  0.2961],
        [ 0.6663, -0.4600,  0.8935,  1.3255,  0.1401],
        [-0.3868,  0.5729,  0.8447, -0.0888,  1.0762],
        [-0.2386,  0.4561,  1.0657,  0.9556,  0.8321],
        [ 1.1891,  0.8166, -0.0999,  0.6712,  0.8294]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0535, -0.5905,  0.2139],
        [ 1.1190,  0.4038, -0.3823],
        [-0.1897, -0.3229,  0.4531]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468]

 avg_word_acc_train = 0.2332175925925926
 avg_word_acc_test = 0.10821759259259259
 avg_letter_acc_train = 0.7492022776514803
 avg_letter_acc_test = 0.6249576767742887

Batch completed Epoch-0 Batch-26 Step-26 TIME ELAPSED = 697.5353372097015

----- Starting Epoch-0 Batch-27 ------
torch.float32
tensor(11232.5879, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11198.6504, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11051.9453, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10842.3174, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10596.2559, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10329.4033, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10050.0088, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9763.1875, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9472.1396, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9179.1162, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8885.2100, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8589.6865, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8290.5312, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7972.0508, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7657.3721, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7361.2202, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7062.1372, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6748.8418, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6395.3613, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5992.9673, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-27 Step-27 TIME ELAPSED = 603.4460134506226
Params after OPT step  params tensor([-0.0021, -0.0066,  0.0314,  ..., -0.0390, -0.0506,  0.4268],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9575, -0.1710,  0.7437,  1.2422,  0.2586],
        [ 0.6119, -0.4170,  0.9009,  1.2901,  0.0655],
        [-0.4496,  0.6319,  0.8377, -0.0878,  1.0254],
        [-0.3098,  0.5248,  1.0163,  1.0579,  0.8394],
        [ 1.1309,  0.9035, -0.1873,  0.7668,  0.8078]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0605, -0.6214,  0.2022],
        [ 1.1453,  0.3855, -0.4655],
        [-0.1772, -0.3307,  0.3783]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193]

 avg_word_acc_train = 0.23325892857142858
 avg_word_acc_test = 0.10602678571428571
 avg_letter_acc_train = 0.7510886320932204
 avg_letter_acc_test = 0.6260713437065354

Batch completed Epoch-0 Batch-27 Step-27 TIME ELAPSED = 682.3871858119965

----- Starting Epoch-0 Batch-28 ------
torch.float32
tensor(10774.8105, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10727.8438, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10495.0352, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10192.5361, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9847.3350, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9484.3691, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9118.9404, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8756.4043, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8396.3730, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8035.1279, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7666.8809, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7281.8096, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6862.4771, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6489.1953, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6138.6133, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5763.9893, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5369.3994, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5037.6187, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4687.3647, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4330.9800, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-28 Step-28 TIME ELAPSED = 605.4679481983185
Params after OPT step  params tensor([ 0.0168, -0.0689, -0.0766,  ..., -0.0397, -0.0551,  0.4777],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0171, -0.1890,  0.6249,  1.1963,  0.3259],
        [ 0.6654, -0.4907,  0.8589,  1.2673,  0.1586],
        [-0.3917,  0.5894,  0.9178, -0.1172,  1.1228],
        [-0.3061,  0.3853,  1.2014,  1.1158,  0.8403],
        [ 1.2642,  0.8282, -0.0868,  0.7339,  0.7988]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0458, -0.6805,  0.1606],
        [ 1.2971,  0.5902, -0.4775],
        [-0.3449, -0.3330,  0.3586]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022]

 avg_word_acc_train = 0.2429956896551724
 avg_word_acc_test = 0.10614224137931035
 avg_letter_acc_train = 0.7560891584384161
 avg_letter_acc_test = 0.6244583716540694

Batch completed Epoch-0 Batch-28 Step-28 TIME ELAPSED = 682.3844695091248

----- Starting Epoch-0 Batch-29 ------
torch.float32
tensor(12033.8828, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11976.7744, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11547.2354, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11041.0283, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10537.2852, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10068.7295, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9637.5283, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9232.6885, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8834.2002, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8420.1377, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8059.4082, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7735.7485, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7405.1938, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7063.4844, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6712.3193, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6367.0371, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6024.6338, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5673.2554, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5308.5635, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4938.1289, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-29 Step-29 TIME ELAPSED = 606.2562165260315
Params after OPT step  params tensor([ 0.0238, -0.0393, -0.0158,  ..., -0.0397, -0.0527,  0.4734],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0768, -0.1269,  0.6402,  1.2294,  0.2984],
        [ 0.7079, -0.4105,  0.9502,  1.3758,  0.0504],
        [-0.4937,  0.5662,  0.8753, -0.0572,  1.0363],
        [-0.3011,  0.4952,  1.1474,  1.1173,  0.7419],
        [ 1.2396,  0.8762, -0.1700,  0.7173,  0.6937]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0078, -0.5924,  0.1905],
        [ 1.2629,  0.4985, -0.5765],
        [-0.3260, -0.3628,  0.5021]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875]

 avg_word_acc_train = 0.2515625
 avg_word_acc_test = 0.109375
 avg_letter_acc_train = 0.7596361864904689
 avg_letter_acc_test = 0.6264946550989338

Batch completed Epoch-0 Batch-29 Step-29 TIME ELAPSED = 683.4808785915375

----- Starting Epoch-0 Batch-30 ------
torch.float32
tensor(11691.8438, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11646.1406, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11312.8340, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10869.3350, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10379.3379, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9867.7109, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9341.5820, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8799.2559, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8225.0254, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7732.9380, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7307.7314, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6909.5962, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6545.9766, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6210.1147, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5860.3135, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5488.6646, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5140.1611, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4814.5020, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4504.4292, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4207.8066, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-30 Step-30 TIME ELAPSED = 603.6180145740509
Params after OPT step  params tensor([ 0.0112, -0.0601, -0.0954,  ..., -0.0379, -0.0571,  0.4690],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1368, -0.1503,  0.6207,  1.1497,  0.1848],
        [ 0.6434, -0.4694,  0.9208,  1.3733,  0.0526],
        [-0.4634,  0.5557,  0.8116, -0.0490,  1.1456],
        [-0.2873,  0.4537,  1.1206,  1.0817,  0.8627],
        [ 1.2753,  0.8872, -0.2970,  0.5824,  0.8386]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0364, -0.6710,  0.2485],
        [ 1.2710,  0.4401, -0.5583],
        [-0.2524, -0.2908,  0.5573]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176]

 avg_word_acc_train = 0.25856854838709675
 avg_word_acc_test = 0.10987903225806452
 avg_letter_acc_train = 0.7634817966897893
 avg_letter_acc_test = 0.6262071635661429

Batch completed Epoch-0 Batch-30 Step-30 TIME ELAPSED = 681.9720239639282

----- Starting Epoch-0 Batch-31 ------
torch.float32
tensor(11009.9717, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10972.5342, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10629.4160, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10225.6357, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9800.3398, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9359.5391, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8907.9316, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8450.0283, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7987.0269, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7512.0928, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7088.4658, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6725.7734, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6386.3198, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6057.3257, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5743.2764, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5446.4150, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5152.2251, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4875.2217, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4600.6904, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4311.9160, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-31 Step-31 TIME ELAPSED = 611.017834186554
Params after OPT step  params tensor([ 0.0294, -0.0341,  0.0020,  ..., -0.0423, -0.0591,  0.5174],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 0.9961, -0.1536,  0.7025,  1.2412,  0.3420],
        [ 0.6040, -0.4722,  0.8953,  1.3258,  0.1212],
        [-0.4633,  0.5813,  0.8463, -0.1668,  1.1271],
        [-0.3154,  0.4166,  1.1852,  1.0841,  0.8239],
        [ 1.2580,  0.8981, -0.1198,  0.7802,  0.7687]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0077, -0.7266,  0.2716],
        [ 1.3336,  0.5238, -0.5058],
        [-0.3039, -0.3265,  0.4165]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368]

 avg_word_acc_train = 0.26708984375
 avg_word_acc_test = 0.10791015625
 avg_letter_acc_train = 0.7675835168590229
 avg_letter_acc_test = 0.6249276633889115

Batch completed Epoch-0 Batch-31 Step-31 TIME ELAPSED = 687.8915410041809

----- Starting Epoch-0 Batch-32 ------
torch.float32
tensor(8979.1465, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8940.9229, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8787.7178, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8568.3018, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8317.3203, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8055.4272, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7789.5654, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7520.6978, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7248.2900, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6969.9580, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6680.5601, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6372.5122, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6036.7700, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5668.3584, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5204.0977, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4899.7905, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4614.9360, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4248.7397, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3983.8982, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3703.3774, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-32 Step-32 TIME ELAPSED = 616.5919134616852
Params after OPT step  params tensor([ 0.0554, -0.0766, -0.0789,  ..., -0.0433, -0.0602,  0.5205],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0211, -0.2343,  0.6299,  1.2695,  0.2867],
        [ 0.6143, -0.5564,  0.8746,  1.4082,  0.0414],
        [-0.4625,  0.4897,  0.7645, -0.1387,  1.1453],
        [-0.2966,  0.4065,  1.1017,  1.0074,  0.8443],
        [ 1.2952,  0.9516, -0.1582,  0.7988,  0.7827]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1029, -0.5775,  0.2381],
        [ 1.4013,  0.4767, -0.5096],
        [-0.2039, -0.3668,  0.5046]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578]

 avg_word_acc_train = 0.2760416666666667
 avg_word_acc_test = 0.10795454545454546
 avg_letter_acc_train = 0.771432912806876
 avg_letter_acc_test = 0.6245223943425977

Batch completed Epoch-0 Batch-32 Step-32 TIME ELAPSED = 694.7752904891968

----- Starting Epoch-0 Batch-33 ------
torch.float32
tensor(14846.8750, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14770.4297, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14400.2285, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13832.5068, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13191.1250, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12536.8242, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11890.6152, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11264.5078, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10660.3896, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10078.8828, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9526.8643, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9032.7656, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8576.7246, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8117.6860, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7649.3286, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7235.1245, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6831.6558, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6401.4761, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6013.8740, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5618.5122, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-33 Step-33 TIME ELAPSED = 614.7569360733032
Params after OPT step  params tensor([ 0.0362, -0.0343, -0.0446,  ..., -0.0418, -0.0572,  0.5076],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0546, -0.1543,  0.6074,  1.2558,  0.2828],
        [ 0.6631, -0.4792,  0.8990,  1.3120,  0.0164],
        [-0.3861,  0.5658,  0.8360, -0.1326,  1.1346],
        [-0.2768,  0.4382,  1.1568,  1.0409,  0.9101],
        [ 1.3347,  0.9269, -0.1256,  0.7210,  0.7964]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0482, -0.6008,  0.1783],
        [ 1.3811,  0.4894, -0.5278],
        [-0.3025, -0.3855,  0.5220]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054]

 avg_word_acc_train = 0.2803308823529412
 avg_word_acc_test = 0.10845588235294118
 avg_letter_acc_train = 0.7729686299181202
 avg_letter_acc_test = 0.6246483017277009

Batch completed Epoch-0 Batch-33 Step-33 TIME ELAPSED = 693.1411211490631

----- Starting Epoch-0 Batch-34 ------
torch.float32
tensor(10496.0566, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10436.8887, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10286.4492, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10060.6055, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9785.0078, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9483.8037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9171.1348, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8851.6143, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8524.6777, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8188.2891, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7841.4277, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7484.6934, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7120.9209, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6753.2422, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6383.3906, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5985.9731, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5629.5068, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5340.0620, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5020.9961, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4676.1841, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-34 Step-34 TIME ELAPSED = 605.7628946304321
Params after OPT step  params tensor([ 0.0303, -0.0693, -0.1002,  ..., -0.0426, -0.0577,  0.5063],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0422, -0.1469,  0.7298,  1.2460,  0.2662],
        [ 0.6709, -0.4563,  0.9462,  1.3236,  0.0221],
        [-0.3520,  0.5731,  0.8604, -0.1519,  1.0928],
        [-0.2926,  0.4454,  1.1483,  1.0066,  0.8864],
        [ 1.2742,  0.8837, -0.1186,  0.7396,  0.8199]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0318, -0.6645,  0.1392],
        [ 1.3464,  0.4374, -0.4395],
        [-0.2474, -0.3761,  0.5685]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256]

 avg_word_acc_train = 0.2857142857142857
 avg_word_acc_test = 0.10803571428571429
 avg_letter_acc_train = 0.7758522063578805
 avg_letter_acc_test = 0.6250382894594931

Batch completed Epoch-0 Batch-34 Step-34 TIME ELAPSED = 681.9009695053101

----- Starting Epoch-0 Batch-35 ------
torch.float32
tensor(9620.9922, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9563.7256, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9294.7285, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8920.6309, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8530.5098, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8154.8916, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7798.1777, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7459.9058, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7141.2690, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6839.1323, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6547.8550, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6258.7188, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5965.2847, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5677.1562, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5400.5293, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5120.3193, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4823.3599, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4495.3667, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4195.0693, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3894.4221, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-35 Step-35 TIME ELAPSED = 605.7059297561646
Params after OPT step  params tensor([ 0.0197, -0.0531, -0.0608,  ..., -0.0452, -0.0572,  0.5137],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0294, -0.1042,  0.6803,  1.2914,  0.3560],
        [ 0.6639, -0.3762,  0.8971,  1.3225,  0.1015],
        [-0.4364,  0.6623,  0.9000, -0.1588,  1.0972],
        [-0.3689,  0.4656,  1.2152,  1.0990,  0.8813],
        [ 1.2080,  0.8752, -0.1533,  0.7266,  0.7603]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0608, -0.6954,  0.1732],
        [ 1.3447,  0.5327, -0.4905],
        [-0.2897, -0.3590,  0.5695]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888]

 avg_word_acc_train = 0.2925347222222222
 avg_word_acc_test = 0.10677083333333333
 avg_letter_acc_train = 0.7789272734548489
 avg_letter_acc_test = 0.6242981584827624

Batch completed Epoch-0 Batch-35 Step-35 TIME ELAPSED = 682.0684607028961

----- Starting Epoch-0 Batch-36 ------
torch.float32
tensor(11678.9678, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11627.6006, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11425.5137, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11156.8984, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10853.4766, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10536.1992, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10217.5410, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9903.4619, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9596.7500, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9299.5479, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9013.3965, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8735.9365, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8459.0840, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8175.0215, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7879.7563, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7571.7466, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7249.1284, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6908.4224, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6508.1421, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6186.0874, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-36 Step-36 TIME ELAPSED = 604.5713868141174
Params after OPT step  params tensor([ 0.0302, -0.0578, -0.0374,  ..., -0.0431, -0.0579,  0.4970],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0246, -0.1930,  0.6034,  1.2501,  0.3107],
        [ 0.6575, -0.4477,  0.9198,  1.3536,  0.0692],
        [-0.4476,  0.5838,  0.8619, -0.1020,  1.1374],
        [-0.3452,  0.4405,  1.1428,  1.0454,  0.8735],
        [ 1.2523,  0.9111, -0.2047,  0.6891,  0.7455]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0043, -0.6102,  0.1438],
        [ 1.3175,  0.4527, -0.5697],
        [-0.3107, -0.3244,  0.5686]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889]

 avg_word_acc_train = 0.2922297297297297
 avg_word_acc_test = 0.109375
 avg_letter_acc_train = 0.7795074892240961
 avg_letter_acc_test = 0.6260438538991442

Batch completed Epoch-0 Batch-36 Step-36 TIME ELAPSED = 680.210239648819

----- Starting Epoch-0 Batch-37 ------
torch.float32
tensor(8960.5547, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8921.7734, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8765.7627, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8551.0498, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8299.6924, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8024.9253, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7736.9458, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7439.6260, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7134.6724, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6814.0605, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6466.9414, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6172.3252, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5889.4639, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5579.6045, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5230.7139, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4913.6812, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4608.6523, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4232.4595, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3976.7659, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3737.6418, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-37 Step-37 TIME ELAPSED = 608.1349511146545
Params after OPT step  params tensor([ 0.0066, -0.1010,  0.0661,  ..., -0.0424, -0.0597,  0.5154],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0106, -0.1797,  0.7782,  1.1573,  0.3007],
        [ 0.6584, -0.5410,  1.0066,  1.3035,  0.0916],
        [-0.2936,  0.5563,  0.8214, -0.1836,  1.2388],
        [-0.3087,  0.4135,  1.1526,  0.9988,  0.9046],
        [ 1.3082,  0.8966, -0.2379,  0.7314,  0.7220]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0654, -0.7634,  0.1150],
        [ 1.3815,  0.5248, -0.5786],
        [-0.4053, -0.0436,  0.4405]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495]

 avg_word_acc_train = 0.2981085526315789
 avg_word_acc_test = 0.10978618421052631
 avg_letter_acc_train = 0.7824251951181753
 avg_letter_acc_test = 0.6260858355194364

Batch completed Epoch-0 Batch-37 Step-37 TIME ELAPSED = 685.6721878051758

----- Starting Epoch-0 Batch-38 ------
torch.float32
tensor(12323.7490, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12259.8145, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11829.2217, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11258.3750, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10669.6602, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10109.5332, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9590.1611, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9110.2744, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8664.5361, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8242.6035, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7836.0142, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7435.3838, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7030.6260, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6609.3672, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6259.1465, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5944.9629, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5618.7607, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5260.4771, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4926.5562, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4618.5869, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-38 Step-38 TIME ELAPSED = 611.0922751426697
Params after OPT step  params tensor([ 0.0057, -0.0507,  0.0052,  ..., -0.0419, -0.0574,  0.5090],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1314, -0.1628,  0.6146,  1.2414,  0.2730],
        [ 0.7342, -0.4602,  0.9016,  1.3775,  0.0128],
        [-0.4229,  0.5548,  0.8408, -0.0526,  1.0877],
        [-0.3263,  0.4563,  1.1258,  1.1200,  0.8518],
        [ 1.2689,  0.9026, -0.2954,  0.6751,  0.7934]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0410, -0.6461,  0.1649],
        [ 1.3604,  0.4547, -0.5604],
        [-0.2425, -0.3680,  0.6839]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664]

 avg_word_acc_train = 0.3004807692307692
 avg_word_acc_test = 0.11017628205128205
 avg_letter_acc_train = 0.7843260202958934
 avg_letter_acc_test = 0.6276470907603731

Batch completed Epoch-0 Batch-38 Step-38 TIME ELAPSED = 690.0527780056

----- Starting Epoch-0 Batch-39 ------
torch.float32
tensor(10776.8418, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10713.6025, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10560.4570, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10326.9492, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10045.6562, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9737.5791, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9414.8389, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9083.0605, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8745.5586, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8400.1045, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8043.1460, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7673.2900, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7337.6372, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7022.9810, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6696.1948, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6345.5674, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5978.9468, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5652.7227, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5323.4946, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4937.5391, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-39 Step-39 TIME ELAPSED = 609.1588745117188
Params after OPT step  params tensor([ 0.0059, -0.0968, -0.0193,  ..., -0.0456, -0.0606,  0.5525],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1973, -0.1311,  0.5920,  1.1802,  0.2555],
        [ 0.7155, -0.5432,  0.8323,  1.3679,  0.0018],
        [-0.4166,  0.5191,  0.7902, -0.0720,  1.1375],
        [-0.3092,  0.4435,  1.1001,  1.1599,  0.9047],
        [ 1.2865,  0.9278, -0.2952,  0.7502,  0.7888]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0172, -0.6924,  0.0971],
        [ 1.3513,  0.5872, -0.5749],
        [-0.3914, -0.3012,  0.6073]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479]

 avg_word_acc_train = 0.304296875
 avg_word_acc_test = 0.108203125
 avg_letter_acc_train = 0.7864360165066427
 avg_letter_acc_test = 0.6272836090516174

Batch completed Epoch-0 Batch-39 Step-39 TIME ELAPSED = 687.8453104496002

----- Starting Epoch-0 Batch-40 ------
torch.float32
tensor(10780.1738, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10737.7256, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10522.2207, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10219.2002, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9873.4346, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9516.1719, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9161.1895, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8809.7979, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8459.3975, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8108.3608, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7756.0278, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7401.0503, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7041.0972, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6672.9834, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6293.3252, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5896.6816, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5437.3403, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5085.6782, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4780.4468, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4424.5864, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-40 Step-40 TIME ELAPSED = 599.489334821701
Params after OPT step  params tensor([ 0.0081, -0.0843,  0.0387,  ..., -0.0463, -0.0612,  0.5570],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.0801, -0.1904,  0.6458,  1.2645,  0.2863],
        [ 0.6477, -0.5799,  0.8679,  1.3618,  0.0149],
        [-0.3678,  0.5311,  0.8345, -0.0998,  1.1728],
        [-0.2673,  0.4357,  1.2223,  1.1331,  0.9254],
        [ 1.3069,  0.8390, -0.2512,  0.6942,  0.7630]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0471, -0.7056,  0.1234],
        [ 1.4058,  0.5624, -0.6066],
        [-0.4418, -0.2648,  0.5936]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375]

 avg_word_acc_train = 0.3094512195121951
 avg_word_acc_test = 0.10746951219512195
 avg_letter_acc_train = 0.788537856713021
 avg_letter_acc_test = 0.627605265239874

Batch completed Epoch-0 Batch-40 Step-40 TIME ELAPSED = 674.9410843849182

----- Starting Epoch-0 Batch-41 ------
torch.float32
tensor(11914.7705, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11860.3662, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11642.8115, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11367.8477, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11068.2637, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10758.6641, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10440.8467, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10117.1846, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9791.9590, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9468.5684, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9148.9082, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8833.1816, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8519.3232, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8202.5938, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7876.3765, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7532.5425, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7162.8042, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6754.7188, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6236.6816, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5794.2783, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-41 Step-41 TIME ELAPSED = 601.6779172420502
Params after OPT step  params tensor([ 0.0124, -0.0801, -0.0166,  ..., -0.0479, -0.0620,  0.5621],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1339, -0.0933,  0.6806,  1.2938,  0.3266],
        [ 0.6950, -0.4962,  0.8538,  1.3688, -0.0074],
        [-0.3922,  0.6346,  0.8632, -0.0863,  1.0351],
        [-0.2994,  0.4771,  1.1938,  1.1684,  0.8531],
        [ 1.2982,  0.9318, -0.3113,  0.6978,  0.7546]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0598, -0.7118,  0.1719],
        [ 1.4196,  0.5291, -0.5207],
        [-0.3099, -0.4385,  0.7152]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849]

 avg_word_acc_train = 0.31138392857142855
 avg_word_acc_test = 0.10602678571428571
 avg_letter_acc_train = 0.789643396563946
 avg_letter_acc_test = 0.6271824847543647

Batch completed Epoch-0 Batch-41 Step-41 TIME ELAPSED = 677.5732178688049

----- Starting Epoch-0 Batch-42 ------
torch.float32
tensor(11979.6885, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11919.8779, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11685.4355, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11341.5068, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10954.4922, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10554.2217, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10142.7402, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9722.4980, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9301.8662, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8887.7744, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8482.5449, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8085.0562, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7692.9951, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7302.4517, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6905.2397, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6472.6729, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6055.3833, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5719.1143, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5362.7812, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5021.9121, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-42 Step-42 TIME ELAPSED = 600.0090003013611
Params after OPT step  params tensor([ 0.0147, -0.1496, -0.0145,  ..., -0.0467, -0.0644,  0.5659],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1149, -0.1009,  0.7653,  1.1107,  0.3839],
        [ 0.7252, -0.5090,  0.9448,  1.2455,  0.0777],
        [-0.3384,  0.6686,  0.8756, -0.1749,  1.1690],
        [-0.3290,  0.4661,  1.1686,  1.0377,  0.9209],
        [ 1.3195,  0.9889, -0.3370,  0.6146,  0.7489]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0582, -0.6835,  0.1616],
        [ 1.4794,  0.5412, -0.5974],
        [-0.3417, -0.2498,  0.4978]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515]

 avg_word_acc_train = 0.3132267441860465
 avg_word_acc_test = 0.10610465116279069
 avg_letter_acc_train = 0.7910918519389153
 avg_letter_acc_test = 0.6277483607255459

Batch completed Epoch-0 Batch-42 Step-42 TIME ELAPSED = 676.5354800224304

----- Starting Epoch-0 Batch-43 ------
torch.float32
tensor(11093.5908, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11038.4766, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10759.6553, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10386.5361, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9966.6338, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9533.1279, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9103.5322, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8683.8701, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8275.7344, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7878.0845, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7487.6328, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7101.9268, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6715.6138, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6360.6431, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6016.3149, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5689.2407, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5356.6240, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5019.8965, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4683.4648, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4340.8052, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-43 Step-43 TIME ELAPSED = 603.5181963443756
Params after OPT step  params tensor([ 0.0238, -0.0912, -0.0747,  ..., -0.0469, -0.0622,  0.5583],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2032, -0.1204,  0.5941,  1.2786,  0.4164],
        [ 0.7718, -0.4859,  0.8096,  1.3464,  0.0545],
        [-0.4114,  0.6543,  0.8613, -0.1262,  1.0871],
        [-0.3456,  0.4090,  1.2061,  1.1298,  0.8833],
        [ 1.3368,  0.8832, -0.3616,  0.6675,  0.7324]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0054, -0.6567,  0.1524],
        [ 1.4873,  0.5463, -0.6099],
        [-0.3977, -0.4003,  0.6319]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489]

 avg_word_acc_train = 0.3171164772727273
 avg_word_acc_test = 0.10688920454545454
 avg_letter_acc_train = 0.7931009598590044
 avg_letter_acc_test = 0.6276496310572186

Batch completed Epoch-0 Batch-43 Step-43 TIME ELAPSED = 679.597907781601

----- Starting Epoch-0 Batch-44 ------
torch.float32
tensor(10641.0615, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10580.8652, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10357.2773, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10038.7275, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9674.0234, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9296.2803, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8922.6875, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8559.8408, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8206.7578, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7858.8291, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7509.9443, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7155.0845, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6791.3286, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6417.6704, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6031.9170, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5605.7241, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5218.0137, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4893.0908, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4541.8994, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4194.1406, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-44 Step-44 TIME ELAPSED = 606.485867023468
Params after OPT step  params tensor([ 0.0320, -0.0833, -0.0813,  ..., -0.0461, -0.0614,  0.5522],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1777, -0.1298,  0.7004,  1.1579,  0.3763],
        [ 0.7609, -0.5553,  0.9474,  1.2627,  0.0544],
        [-0.2931,  0.6195,  0.8979, -0.1822,  1.1107],
        [-0.2827,  0.4205,  1.2692,  1.0931,  0.8736],
        [ 1.3578,  0.8938, -0.2559,  0.7390,  0.7342]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[ 0.0263, -0.7774,  0.1041],
        [ 1.5292,  0.4837, -0.5116],
        [-0.3039, -0.3072,  0.6833]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449]

 avg_word_acc_train = 0.3215277777777778
 avg_word_acc_test = 0.10902777777777778
 avg_letter_acc_train = 0.7952950881886682
 avg_letter_acc_test = 0.6294339487055837

Batch completed Epoch-0 Batch-44 Step-44 TIME ELAPSED = 684.0279622077942

----- Starting Epoch-0 Batch-45 ------
torch.float32
tensor(12863.6865, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12806.6758, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12580.0342, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12236.1602, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11836.5244, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11415.8379, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10994.0947, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10582.0811, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10184.3105, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9799.9062, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9422.0596, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9035.2559, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8646.1709, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8305.6719, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7976.9380, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7613.8096, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7215.4868, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6827.7402, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6417.9780, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5965.5654, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-45 Step-45 TIME ELAPSED = 608.886791229248
Params after OPT step  params tensor([ 0.0345, -0.1060, -0.1230,  ..., -0.0496, -0.0647,  0.5885],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1991, -0.0540,  0.6507,  1.1500,  0.4246],
        [ 0.7022, -0.5251,  0.8391,  1.2596,  0.0454],
        [-0.3688,  0.7303,  0.8403, -0.1470,  1.1518],
        [-0.2869,  0.5161,  1.2577,  1.1420,  0.9241],
        [ 1.3549,  0.9800, -0.3195,  0.6446,  0.8262]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1365, -0.7060,  0.2489],
        [ 1.5320,  0.6200, -0.5930],
        [-0.4175, -0.4592,  0.5302]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459]

 avg_word_acc_train = 0.3226902173913043
 avg_word_acc_test = 0.10767663043478261
 avg_letter_acc_train = 0.796013527866594
 avg_letter_acc_test = 0.6279053462657437

Batch completed Epoch-0 Batch-45 Step-45 TIME ELAPSED = 685.9222049713135

----- Starting Epoch-0 Batch-46 ------
torch.float32
tensor(8678.8584, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8628.6514, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8356.5469, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8006.0020, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7625.3608, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7239.6411, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6857.3823, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6479.4365, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6102.3892, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5723.7427, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5345.0547, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4967.3013, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4574.2100, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4196.0811, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3887.6741, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3576.9363, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3280.9209, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3027.7434, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2761.8435, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2511.2959, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-46 Step-46 TIME ELAPSED = 606.9442391395569
Params after OPT step  params tensor([ 0.0178, -0.1083, -0.0710,  ..., -0.0489, -0.0649,  0.5759],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2402, -0.0985,  0.6545,  1.3217,  0.3237],
        [ 0.6735, -0.5210,  0.8668,  1.3605, -0.0859],
        [-0.2966,  0.6902,  0.8465, -0.0768,  1.1367],
        [-0.2907,  0.5421,  1.2229,  1.1042,  0.9217],
        [ 1.3702,  0.9424, -0.3335,  0.6696,  0.8112]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0556, -0.7921,  0.1988],
        [ 1.6868,  0.5896, -0.5838],
        [-0.2721, -0.4213,  0.5936]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566]

 avg_word_acc_train = 0.33011968085106386
 avg_word_acc_test = 0.10804521276595745
 avg_letter_acc_train = 0.7991182503347721
 avg_letter_acc_test = 0.6285151401019121

Batch completed Epoch-0 Batch-46 Step-46 TIME ELAPSED = 684.5066330432892

----- Starting Epoch-0 Batch-47 ------
torch.float32
tensor(12693.0967, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12626.8203, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12290.4814, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11793.3848, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11240.5117, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10683.3643, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10143.6787, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9628.5645, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9136.3311, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8666.5059, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8206.1826, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7759.7847, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7317.8916, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6941.2085, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6590.8223, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6229.3408, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5859.6392, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5516.4282, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5198., device='cuda:0', grad_fn=<AddBackward0>)
tensor(4847.9131, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-47 Step-47 TIME ELAPSED = 607.7205889225006
Params after OPT step  params tensor([ 0.0206, -0.1095, -0.0466,  ..., -0.0485, -0.0653,  0.5817],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2100, -0.0714,  0.6825,  1.1929,  0.3447],
        [ 0.6737, -0.5654,  0.9385,  1.3304, -0.0072],
        [-0.2914,  0.6701,  0.8764, -0.1566,  1.1534],
        [-0.2849,  0.4776,  1.2718,  1.0976,  0.9662],
        [ 1.3473,  0.9393, -0.2455,  0.6294,  0.8032]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0693, -0.7522,  0.2511],
        [ 1.5474,  0.5728, -0.6154],
        [-0.3668, -0.3895,  0.5721]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398]

 avg_word_acc_train = 0.3330078125
 avg_word_acc_test = 0.11002604166666667
 avg_letter_acc_train = 0.8007556207769265
 avg_letter_acc_test = 0.6282851523530252

Batch completed Epoch-0 Batch-47 Step-47 TIME ELAPSED = 684.4608345031738

----- Starting Epoch-0 Batch-48 ------
torch.float32
tensor(10269.5322, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10230.7949, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10086.5020, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9863.5791, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9583.8564, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9262.9111, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8915.5225, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8553.2256, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8179.7632, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7791.3633, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7366.4380, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6899.4902, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6490.5591, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6094.2656, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5678.7983, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5333.3740, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4990.3726, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4611.1226, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4243.6948, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3890.3896, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-48 Step-48 TIME ELAPSED = 605.1635129451752
Params after OPT step  params tensor([ 0.0180, -0.0590, -0.0758,  ..., -0.0492, -0.0669,  0.6136],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1807, -0.0773,  0.6223,  1.2035,  0.4213],
        [ 0.6997, -0.4444,  0.9596,  1.3416,  0.0516],
        [-0.3415,  0.6876,  0.8763, -0.2865,  1.0896],
        [-0.3256,  0.4252,  1.2566,  1.1055,  0.9403],
        [ 1.3438,  1.0217, -0.1832,  0.6129,  0.9151]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-1.3488e-03, -7.5759e-01,  1.9330e-01],
        [ 1.5134e+00,  6.9740e-01, -6.0254e-01],
        [-3.7273e-01, -4.2244e-01,  6.2096e-01]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599]

 avg_word_acc_train = 0.33737244897959184
 avg_word_acc_test = 0.11001275510204081
 avg_letter_acc_train = 0.8030472097051703
 avg_letter_acc_test = 0.6274205844489178

Batch completed Epoch-0 Batch-48 Step-48 TIME ELAPSED = 680.784234046936

----- Starting Epoch-0 Batch-49 ------
torch.float32
tensor(10575.0381, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10525.4756, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10045.0771, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9466.0332, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8885.2422, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8330.5547, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7804.1548, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7295.2461, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6765.5952, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6324.6372, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5914.8018, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5501.4414, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5074.7559, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4692.7021, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4381.9331, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4059.1558, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3775.4788, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3499.1985, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3222.3455, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3001.0935, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-49 Step-49 TIME ELAPSED = 602.2492878437042
Params after OPT step  params tensor([ 0.0229, -0.0458, -0.0706,  ..., -0.0482, -0.0646,  0.5880],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1852, -0.0962,  0.6913,  1.1011,  0.3947],
        [ 0.7104, -0.5155,  0.9724,  1.2533, -0.0376],
        [-0.3595,  0.7279,  0.9666, -0.1980,  1.0803],
        [-0.3818,  0.5501,  1.3426,  1.1244,  0.9236],
        [ 1.2953,  1.0338, -0.2408,  0.6289,  0.8323]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0851, -0.7167,  0.2299],
        [ 1.6253,  0.6569, -0.5870],
        [-0.3187, -0.3950,  0.6030]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489]

 avg_word_acc_train = 0.34375
 avg_word_acc_test = 0.1103125
 avg_letter_acc_train = 0.8056592512930574
 avg_letter_acc_test = 0.6276344853937723

Batch completed Epoch-0 Batch-49 Step-49 TIME ELAPSED = 679.1028671264648

----- Starting Epoch-0 Batch-50 ------
torch.float32
tensor(11714.7686, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11652.4209, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11440.0596, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11125.1113, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10758.7236, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10375.8223, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9996.2822, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9630.5371, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9281.7500, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8949.3018, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8629.2529, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8316.9951, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8007.4702, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7695.4590, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7375.0376, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7039.7256, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6683.0479, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6287.5723, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5837.4316, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5480.0425, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-50 Step-50 TIME ELAPSED = 607.1512916088104
Params after OPT step  params tensor([ 0.0285, -0.0312, -0.0721,  ..., -0.0490, -0.0656,  0.5967],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1995, -0.0875,  0.6720,  1.2069,  0.3749],
        [ 0.6448, -0.5217,  0.9350,  1.3474, -0.0161],
        [-0.4033,  0.7044,  0.8886, -0.1913,  1.0958],
        [-0.3536,  0.5199,  1.2954,  1.0985,  0.9287],
        [ 1.3150,  1.0177, -0.1960,  0.6396,  0.8296]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1065, -0.7080,  0.2240],
        [ 1.5624,  0.6690, -0.6609],
        [-0.4057, -0.4214,  0.5471]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014]

 avg_word_acc_train = 0.34558823529411764
 avg_word_acc_test = 0.11151960784313726
 avg_letter_acc_train = 0.8062351591088438
 avg_letter_acc_test = 0.6286914872909554

Batch completed Epoch-0 Batch-50 Step-50 TIME ELAPSED = 683.792005777359

----- Starting Epoch-0 Batch-51 ------
torch.float32
tensor(10698.9219, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10653.9873, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10512.7441, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10311.4463, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10061.0029, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9775.4521, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9464.5137, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9136.5625, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8796.4863, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8444.5459, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8077.9673, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7687.4434, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7277.1274, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6864.8594, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6482.1343, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6106.7900, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5725.6675, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5344.3970, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5002.5312, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4689.6934, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-51 Step-51 TIME ELAPSED = 599.5324585437775
Params after OPT step  params tensor([ 0.0425, -0.0424, -0.0895,  ..., -0.0494, -0.0666,  0.6065],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2264, -0.0439,  0.7304,  1.3174,  0.3696],
        [ 0.6383, -0.5501,  0.9354,  1.4389,  0.0145],
        [-0.3327,  0.6389,  0.7183, -0.2616,  1.2006],
        [-0.2701,  0.4071,  1.1260,  1.0299,  0.9565],
        [ 1.4441,  0.9653, -0.1837,  0.7149,  0.6965]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0892, -0.7530,  0.2529],
        [ 1.5217,  0.6473, -0.6475],
        [-0.4746, -0.3420,  0.6044]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143]

 avg_word_acc_train = 0.34585336538461536
 avg_word_acc_test = 0.11057692307692307
 avg_letter_acc_train = 0.80726387733343
 avg_letter_acc_test = 0.6284144531947007

Batch completed Epoch-0 Batch-51 Step-51 TIME ELAPSED = 674.4580090045929

----- Starting Epoch-0 Batch-52 ------
torch.float32
tensor(13120.7012, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13063.2363, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12423.2764, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11626.6885, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10821.4541, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10060.9590, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9373.7432, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8776.4707, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8200.5195, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7677.1523, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7255.1035, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6877.0112, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6489.2148, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6112.4824, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5772.9473, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5413.4478, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5062.5112, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4699.0112, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4394.9834, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4136.3774, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-52 Step-52 TIME ELAPSED = 606.6066343784332
Params after OPT step  params tensor([ 0.0483,  0.0226,  0.0289,  ..., -0.0498, -0.0659,  0.6079],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.1891, -0.0311,  0.7136,  1.1374,  0.4262],
        [ 0.7804, -0.4660,  0.9851,  1.2625, -0.0290],
        [-0.3467,  0.6742,  0.8863, -0.2063,  1.1299],
        [-0.3278,  0.4103,  1.2674,  1.1286,  0.9772],
        [ 1.3543,  0.9085, -0.2512,  0.6540,  0.8016]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0384, -0.7062,  0.1748],
        [ 1.6218,  0.6600, -0.6004],
        [-0.3125, -0.3959,  0.5971]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921]

 avg_word_acc_train = 0.3487617924528302
 avg_word_acc_test = 0.11143867924528301
 avg_letter_acc_train = 0.8087829655177464
 avg_letter_acc_test = 0.6296725946217308

Batch completed Epoch-0 Batch-52 Step-52 TIME ELAPSED = 683.7363359928131

----- Starting Epoch-0 Batch-53 ------
torch.float32
tensor(11170.8486, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11094.6201, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10902.8018, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10615.2734, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10271.7178, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9894.4785, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9495.9707, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9083.1084, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8663.7861, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8246.0654, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7831.7080, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7414.7769, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6985.3042, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6502.7949, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6084.0088, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5720.6924, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5333.6870, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4926.8457, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4499.7671, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4110.5537, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-0 Batch-53 Step-53 TIME ELAPSED = 439.03270721435547
Params after OPT step  params tensor([ 0.0506,  0.0263,  0.0266,  ..., -0.0524, -0.0676,  0.6314],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2303, -0.0121,  0.7956,  1.2590,  0.4239],
        [ 0.7239, -0.4478,  0.9634,  1.2786, -0.0809],
        [-0.3420,  0.6987,  0.8276, -0.1819,  1.1112],
        [-0.3232,  0.4885,  1.3237,  1.1500,  0.9469],
        [ 1.3617,  0.9094, -0.2549,  0.6579,  0.8716]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1578, -0.7683,  0.1951],
        [ 1.6145,  0.7108, -0.6396],
        [-0.4729, -0.3426,  0.6154]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295]

 avg_word_acc_train = 0.351965076489533
 avg_word_acc_test = 0.11053240740740741
 avg_letter_acc_train = 0.8106113263575485
 avg_letter_acc_test = 0.6289409527662585

Batch completed Epoch-0 Batch-53 Step-53 TIME ELAPSED = 503.29660820961
Epoch completed Epoch-0 Batch-53 Step-53 TIME ELAPSED = 37010.15950989723

--------------Starting Epoch 1-------------------
Loaded dataset... 

----- Starting Epoch-1 Batch-0 ------
torch.float32
tensor(10742.5449, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10677.6680, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10417.2109, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10047.5029, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9623.2871, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9185.8496, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8758.0996, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8347.4434, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7951.0269, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7561.3818, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7170.5220, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6772.3818, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6356.7661, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5909.3823, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5531.3828, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5215.3901, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4874.2241, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4588.1704, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4324.4116, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3992.2280, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-0 Step-54 TIME ELAPSED = 606.6047160625458
Params after OPT step  params tensor([ 0.0292,  0.0026, -0.0176,  ..., -0.0504, -0.0676,  0.6347],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2149,  0.0363,  0.8753,  1.2303,  0.4674],
        [ 0.7000, -0.4783,  1.0612,  1.2895, -0.0754],
        [-0.3381,  0.6738,  0.8968, -0.2033,  1.1453],
        [-0.3457,  0.4468,  1.3959,  1.1534,  0.8590],
        [ 1.3640,  0.9277, -0.2677,  0.6365,  0.7464]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1731, -0.7719,  0.2084],
        [ 1.6949,  0.7142, -0.6497],
        [-0.4412, -0.3893,  0.6130]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375]

 avg_word_acc_train = 0.35579298418972327
 avg_word_acc_test = 0.11051136363636363
 avg_letter_acc_train = 0.8121575235858302
 avg_letter_acc_test = 0.6292599240795993

Batch completed Epoch-1 Batch-0 Step-54 TIME ELAPSED = 682.9967522621155

----- Starting Epoch-1 Batch-1 ------
torch.float32
tensor(10566.8203, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10511.9209, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10260.9844, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9892.9414, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9470.9404, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9030.0195, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8588.8477, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8157.1562, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7743.6123, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7346.0903, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6948.0342, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6558.3848, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6248.8940, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5925.4136, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5562.1841, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5141.9399, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4810.5957, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4503.2466, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4179.1562, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3909.5559, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-1 Step-55 TIME ELAPSED = 605.306266784668
Params after OPT step  params tensor([ 0.0368, -0.0492, -0.0445,  ..., -0.0493, -0.0676,  0.6298],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2188, -0.0190,  0.7423,  1.2462,  0.4952],
        [ 0.7544, -0.4663,  1.0091,  1.3265, -0.0057],
        [-0.3173,  0.6905,  0.8834, -0.2637,  1.1423],
        [-0.3292,  0.4106,  1.3007,  1.1238,  0.9590],
        [ 1.3859,  0.9370, -0.3154,  0.6604,  0.8795]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0519, -0.7622,  0.1143],
        [ 1.6624,  0.7203, -0.6762],
        [-0.3704, -0.4278,  0.7681]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593]

 avg_word_acc_train = 0.3594841809006211
 avg_word_acc_test = 0.11188616071428571
 avg_letter_acc_train = 0.8138917498351615
 avg_letter_acc_test = 0.6299856917321307

Batch completed Epoch-1 Batch-1 Step-55 TIME ELAPSED = 681.9796288013458

----- Starting Epoch-1 Batch-2 ------
torch.float32
tensor(10277.9053, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10225.9014, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9988.7061, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9634.3721, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9217.5322, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8760.7197, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8271.3145, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7745.0859, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7177.1489, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6641.5767, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6173.0898, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5730.1602, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5264.7100, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4900.4956, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4587.4980, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4266.1265, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3952.2334, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3648.5300, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3383.5056, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3131.1355, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-2 Step-56 TIME ELAPSED = 609.8119192123413
Params after OPT step  params tensor([ 0.0341, -0.0379, -0.0505,  ..., -0.0507, -0.0691,  0.6251],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3095e+00, -1.1137e-03,  7.6123e-01,  1.2448e+00,  4.4331e-01],
        [ 6.8299e-01, -5.2193e-01,  9.8129e-01,  1.3579e+00,  5.9993e-03],
        [-2.4690e-01,  7.8848e-01,  8.0099e-01, -3.6148e-01,  1.1223e+00],
        [-3.0621e-01,  5.9985e-01,  1.3254e+00,  1.0772e+00,  9.4035e-01],
        [ 1.4377e+00,  1.0510e+00, -2.5785e-01,  6.1752e-01,  8.7363e-01]],
       device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1305, -0.8470,  0.2609],
        [ 1.7631,  0.6326, -0.6885],
        [-0.4037, -0.4256,  0.7738]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814]

 avg_word_acc_train = 0.36414235316552246
 avg_word_acc_test = 0.1112938596491228
 avg_letter_acc_train = 0.8158684888684047
 avg_letter_acc_test = 0.629235228760086

Batch completed Epoch-1 Batch-2 Step-56 TIME ELAPSED = 687.079108953476

----- Starting Epoch-1 Batch-3 ------
torch.float32
tensor(9811.3105, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9757.1582, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9519.3945, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9209.8252, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8867.4717, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8505.1758, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8123.0874, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7720.0112, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7297.7017, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6862.7173, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6423.1958, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5981.5640, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5523.3081, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5000.9517, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4644.9272, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4323.3511, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3937.5105, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3643.0645, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3349.9202, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3008.9902, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-3 Step-57 TIME ELAPSED = 609.9079556465149
Params after OPT step  params tensor([ 0.0411,  0.0169, -0.0351,  ..., -0.0512, -0.0694,  0.6287],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2712, -0.0427,  0.7406,  1.2810,  0.3620],
        [ 0.6476, -0.5350,  0.9751,  1.3956,  0.0086],
        [-0.3298,  0.6983,  0.8648, -0.3132,  1.1601],
        [-0.2915,  0.4889,  1.3502,  1.1372,  0.9807],
        [ 1.4352,  1.0223, -0.2601,  0.5555,  0.8490]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0152, -0.7808,  0.1155],
        [ 1.8190,  0.7396, -0.6911],
        [-0.3952, -0.4656,  0.7395]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746]

 avg_word_acc_train = 0.3681011056971514
 avg_word_acc_test = 0.11099137931034483
 avg_letter_acc_train = 0.8174582002478964
 avg_letter_acc_test = 0.6289436910872841

Batch completed Epoch-1 Batch-3 Step-57 TIME ELAPSED = 686.9580903053284

----- Starting Epoch-1 Batch-4 ------
torch.float32
tensor(12446.0742, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12373.8887, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11827.1484, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11148.7793, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10430.5645, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9734.9678, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9074.6025, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8427.5459, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7812.1577, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7306.3408, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6835.0986, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6337.8550, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5862.1533, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5460.9316, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5074.0820, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4680.7114, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4328.9961, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3982.4155, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3632.6892, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3366.8770, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-4 Step-58 TIME ELAPSED = 611.1163103580475
Params after OPT step  params tensor([ 0.0505,  0.0172,  0.0023,  ..., -0.0495, -0.0694,  0.6282],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2520, -0.0244,  0.7515,  1.3493,  0.4572],
        [ 0.6610, -0.5948,  0.9012,  1.3672, -0.0179],
        [-0.2363,  0.7526,  0.7724, -0.2957,  1.2358],
        [-0.3046,  0.5652,  1.2841,  1.1627,  0.9899],
        [ 1.4682,  1.0783, -0.3651,  0.6285,  0.8396]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0485, -0.8801,  0.2829],
        [ 1.8242,  0.6723, -0.6366],
        [-0.4872, -0.5218,  0.8117]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463]

 avg_word_acc_train = 0.37166083271923356
 avg_word_acc_test = 0.1114936440677966
 avg_letter_acc_train = 0.8187268357065369
 avg_letter_acc_test = 0.6290727664889393

Batch completed Epoch-1 Batch-4 Step-58 TIME ELAPSED = 688.4665699005127

----- Starting Epoch-1 Batch-5 ------
torch.float32
tensor(10374.0342, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10330.2627, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10090.6738, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9719.2178, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9294.7119, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8866.9375, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8448.2891, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8025.7344, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7589.6436, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7129.2749, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6620.8169, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6174.8735, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5799.1597, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5385.2803, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4973.9150, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4618.1953, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4209.9297, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3797.1855, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3404.3943, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3053.3594, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-5 Step-59 TIME ELAPSED = 613.2700006961823
Params after OPT step  params tensor([ 0.0446, -0.0530, -0.0382,  ..., -0.0503, -0.0693,  0.6396],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2503, -0.0591,  0.7148,  1.3153,  0.4126],
        [ 0.6745, -0.5972,  0.9471,  1.3623, -0.0333],
        [-0.3670,  0.6988,  0.8556, -0.2331,  1.2493],
        [-0.3958,  0.5265,  1.3780,  1.2188,  1.0007],
        [ 1.3820,  0.9735, -0.3541,  0.5916,  0.8658]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1016, -0.7332,  0.1430],
        [ 1.8624,  0.6892, -0.6618],
        [-0.5296, -0.5491,  0.8390]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872]

 avg_word_acc_train = 0.37562273550724634
 avg_word_acc_test = 0.11223958333333334
 avg_letter_acc_train = 0.8205939478473411
 avg_letter_acc_test = 0.6292123903675101

Batch completed Epoch-1 Batch-5 Step-59 TIME ELAPSED = 690.1316242218018

----- Starting Epoch-1 Batch-6 ------
torch.float32
tensor(12888.0732, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12830.4717, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12529.6211, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12103.5459, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11609.9678, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11085.5449, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10549.1660, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10009.9502, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9476.0811, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8953.1826, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8440.7344, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7933.7183, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7424.6484, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6845.6304, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6411.3135, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5995.8867, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5521.2695, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4990.6948, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4545.2051, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4159.7319, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-6 Step-60 TIME ELAPSED = 608.8308894634247
Params after OPT step  params tensor([ 0.0433,  0.0147,  0.0184,  ..., -0.0513, -0.0699,  0.6422],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3466, -0.0164,  0.7736,  1.3387,  0.4515],
        [ 0.7006, -0.5464,  0.9755,  1.3972, -0.0996],
        [-0.3395,  0.7248,  0.8211, -0.2151,  1.0860],
        [-0.4427,  0.4930,  1.3470,  1.2864,  0.8837],
        [ 1.3739,  0.9907, -0.3644,  0.6971,  0.7812]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1711, -0.8006,  0.2212],
        [ 1.8483,  0.7091, -0.6571],
        [-0.4980, -0.5150,  0.8175]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885]

 avg_word_acc_train = 0.3768932644333571
 avg_word_acc_test = 0.11321721311475409
 avg_letter_acc_train = 0.8219231231312759
 avg_letter_acc_test = 0.6300937374055328

Batch completed Epoch-1 Batch-6 Step-60 TIME ELAPSED = 686.0900480747223

----- Starting Epoch-1 Batch-7 ------
torch.float32
tensor(9396.0244, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9342.7246, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9173.3105, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8900.1465, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8571.7852, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8219.9092, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7862.5913, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7506.7056, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7151.6885, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6793.6895, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6427.6055, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6048.3062, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5652.7964, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5243.3208, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4827.6846, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4389.7197, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4026.4014, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3752.7764, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3479.8721, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3269.1433, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-7 Step-61 TIME ELAPSED = 607.4713795185089
Params after OPT step  params tensor([ 0.0451,  0.0116, -0.0072,  ..., -0.0512, -0.0712,  0.6481],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2631, -0.0521,  0.7765,  1.3214,  0.4752],
        [ 0.6514, -0.5978,  1.0170,  1.4212, -0.1058],
        [-0.3915,  0.7258,  0.8560, -0.1992,  1.1596],
        [-0.4477,  0.5022,  1.3267,  1.2996,  0.9131],
        [ 1.4054,  1.0006, -0.3608,  0.6961,  0.7585]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0775, -0.7904,  0.1901],
        [ 1.9396,  0.6765, -0.7076],
        [-0.4626, -0.5398,  0.7961]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115]

 avg_word_acc_train = 0.3808949859747545
 avg_word_acc_test = 0.11315524193548387
 avg_letter_acc_train = 0.8235691470837898
 avg_letter_acc_test = 0.6309617101244276

Batch completed Epoch-1 Batch-7 Step-61 TIME ELAPSED = 684.119104385376

----- Starting Epoch-1 Batch-8 ------
torch.float32
tensor(13630.6143, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13584.2168, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13353.6475, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13017.0400, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12623.2500, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12190.5098, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11729.8584, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11249.9502, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10757.6123, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10248.3291, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9704.6475, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9138.1445, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8721.6807, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8308.7744, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7837.9946, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7290.7368, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6795.4922, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6357.2783, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5880.4722, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5491.3242, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-8 Step-62 TIME ELAPSED = 608.004275560379
Params after OPT step  params tensor([ 0.0385, -0.0268, -0.0354,  ..., -0.0532, -0.0733,  0.6585],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3753,  0.0282,  0.7443,  1.2903,  0.4243],
        [ 0.6933, -0.5440,  1.0482,  1.4704, -0.0984],
        [-0.3956,  0.7016,  0.8320, -0.2319,  1.1906],
        [-0.4120,  0.4526,  1.2992,  1.1967,  0.9057],
        [ 1.4501,  0.9610, -0.3363,  0.5829,  0.7691]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2310, -0.7294,  0.1979],
        [ 1.8086,  0.7349, -0.7528],
        [-0.5383, -0.5005,  0.8037]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619]

 avg_word_acc_train = 0.3832815734989648
 avg_word_acc_test = 0.11235119047619048
 avg_letter_acc_train = 0.8243466053778223
 avg_letter_acc_test = 0.6303566220797694

Batch completed Epoch-1 Batch-8 Step-62 TIME ELAPSED = 684.9313519001007

----- Starting Epoch-1 Batch-9 ------
torch.float32
tensor(11353.9131, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11293.1133, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11006.5137, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10604.4639, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10140.2207, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9654.0967, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9180.4863, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8740.5596, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8331.9287, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7940.7334, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7554.3970, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7162.4185, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6757.3345, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6337.8311, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5908.6631, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5468.0278, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4953.4541, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4597.0522, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4304.3994, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3960.3254, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-9 Step-63 TIME ELAPSED = 596.8428754806519
Params after OPT step  params tensor([ 0.0463,  0.0354, -0.0009,  ..., -0.0530, -0.0735,  0.6711],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4076e+00,  9.0580e-04,  8.1205e-01,  1.3690e+00,  3.7544e-01],
        [ 6.8747e-01, -5.6532e-01,  1.0508e+00,  1.4991e+00, -1.2504e-01],
        [-3.4786e-01,  6.4237e-01,  8.1036e-01, -2.5896e-01,  1.1095e+00],
        [-4.1152e-01,  4.1362e-01,  1.4057e+00,  1.2527e+00,  8.4613e-01],
        [ 1.4417e+00,  9.4209e-01, -2.9912e-01,  5.4181e-01,  7.0239e-01]],
       device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1351, -0.7779,  0.2293],
        [ 1.8786,  0.7601, -0.7493],
        [-0.5229, -0.4898,  0.8099]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079]

 avg_word_acc_train = 0.38583772078804346
 avg_word_acc_test = 0.113525390625
 avg_letter_acc_train = 0.8255163378901378
 avg_letter_acc_test = 0.63095631642292

Batch completed Epoch-1 Batch-9 Step-63 TIME ELAPSED = 672.5098073482513

----- Starting Epoch-1 Batch-10 ------
torch.float32
tensor(11712.8076, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11630.7666, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11494.3379, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11257.5400, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10939.5098, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10562.4893, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10146.5420, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9706.4268, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9253.2285, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8795.6709, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8336.9531, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7874.6245, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7405.9497, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6917.6909, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6390.4731, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5961.2964, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5582.1230, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5185.2285, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4812.0728, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4513.5273, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-10 Step-64 TIME ELAPSED = 602.464391708374
Params after OPT step  params tensor([ 0.0438, -0.0506, -0.0682,  ..., -0.0521, -0.0725,  0.6619],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3510, -0.0092,  0.7294,  1.3238,  0.4399],
        [ 0.6580, -0.5583,  1.0250,  1.4333, -0.0892],
        [-0.4012,  0.7068,  0.8787, -0.2344,  1.1561],
        [-0.4462,  0.4550,  1.3440,  1.2236,  0.9391],
        [ 1.4278,  0.9694, -0.3096,  0.5858,  0.7810]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.0641, -0.7644,  0.2045],
        [ 1.8949,  0.6751, -0.7443],
        [-0.4693, -0.5642,  0.8248]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418]

 avg_word_acc_train = 0.386632525083612
 avg_word_acc_test = 0.11322115384615385
 avg_letter_acc_train = 0.8264291401510354
 avg_letter_acc_test = 0.6310114215950358

Batch completed Epoch-1 Batch-10 Step-64 TIME ELAPSED = 679.0628411769867

----- Starting Epoch-1 Batch-11 ------
torch.float32
tensor(12042.1543, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11980.0752, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11717.7178, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11327.3359, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10875.4102, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10393.7236, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9904.6270, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9425.7559, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8968.7090, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8543.5361, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8153.7197, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7776.2139, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7391.4600, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6994.8691, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6586.5986, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6160.8760, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5676.6152, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5294.7480, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4993.0513, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4663.1382, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-11 Step-65 TIME ELAPSED = 603.6488313674927
Params after OPT step  params tensor([ 0.0536, -0.0284, -0.0360,  ..., -0.0526, -0.0730,  0.6594],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3971,  0.0277,  0.7803,  1.3469,  0.4200],
        [ 0.7119, -0.5412,  0.9824,  1.4063, -0.1444],
        [-0.3773,  0.7264,  0.8311, -0.2042,  1.1562],
        [-0.5034,  0.4461,  1.3083,  1.2291,  0.9147],
        [ 1.3843,  1.0486, -0.3036,  0.6321,  0.8310]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1178, -0.8418,  0.2141],
        [ 1.8691,  0.7105, -0.7265],
        [-0.4580, -0.5068,  0.8667]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575]

 avg_word_acc_train = 0.38669301712779974
 avg_word_acc_test = 0.11268939393939394
 avg_letter_acc_train = 0.8272781877443564
 avg_letter_acc_test = 0.6314116385795694

Batch completed Epoch-1 Batch-11 Step-65 TIME ELAPSED = 679.3435206413269

----- Starting Epoch-1 Batch-12 ------
torch.float32
tensor(11558.5391, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11507.0635, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11230.3389, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10840.8447, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10393.6367, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9916.9365, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9420.9912, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8897.0605, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8339.6025, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7836.8032, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7376.9141, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6890.8516, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6499.4600, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6094.2861, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5656.4512, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5265.0327, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4945.5098, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4632.5259, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4284.0977, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4001.0872, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-12 Step-66 TIME ELAPSED = 600.066020488739
Params after OPT step  params tensor([ 0.0844,  0.0087, -0.0927,  ..., -0.0535, -0.0761,  0.6769],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3733, -0.0183,  0.8254,  1.3839,  0.4467],
        [ 0.7419, -0.5238,  1.0010,  1.4215, -0.1543],
        [-0.3175,  0.6519,  0.7230, -0.2702,  1.1144],
        [-0.4434,  0.5042,  1.3967,  1.2557,  0.8104],
        [ 1.4415,  1.0815, -0.2719,  0.6482,  0.7606]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1610, -0.8444,  0.2233],
        [ 1.8946,  0.7858, -0.7606],
        [-0.3714, -0.4208,  0.8463]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777]

 avg_word_acc_train = 0.3879177482154445
 avg_word_acc_test = 0.1117070895522388
 avg_letter_acc_train = 0.828331954188456
 avg_letter_acc_test = 0.6309096406571546

Batch completed Epoch-1 Batch-12 Step-66 TIME ELAPSED = 675.6525821685791

----- Starting Epoch-1 Batch-13 ------
torch.float32
tensor(13330.2373, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13262.0391, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12936.2686, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12511.4580, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12052.2871, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11590.9834, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11121.9795, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10652.3184, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10190.7627, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9744.4531, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9314.2344, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8890.5898, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8451.3125, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7988.1172, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7586.8330, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7206.5728, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6782.9487, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6388.7666, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5979.2397, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5550.5063, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-13 Step-67 TIME ELAPSED = 577.3354992866516
Params after OPT step  params tensor([ 0.0628, -0.0267, -0.0924,  ..., -0.0527, -0.0748,  0.6750],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3336,  0.0099,  0.7055,  1.3853,  0.5093],
        [ 0.6966, -0.5292,  0.9651,  1.4660, -0.0939],
        [-0.3249,  0.7615,  0.7425, -0.2490,  1.1373],
        [-0.4470,  0.5240,  1.3220,  1.2543,  0.8544],
        [ 1.4347,  1.0697, -0.3397,  0.6704,  0.7828]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1706, -0.8182,  0.2383],
        [ 1.8607,  0.7558, -0.7077],
        [-0.5121, -0.5341,  0.8475]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682]

 avg_word_acc_train = 0.3881873401534527
 avg_word_acc_test = 0.11213235294117647
 avg_letter_acc_train = 0.8288901610214551
 avg_letter_acc_test = 0.6319379815310741

Batch completed Epoch-1 Batch-13 Step-67 TIME ELAPSED = 647.2575769424438

----- Starting Epoch-1 Batch-14 ------
torch.float32
tensor(8834.1777, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8794.1592, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8634.9541, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8395.6504, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8103.8418, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7781.8032, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7445.8589, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7104.9888, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6762.8652, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6420.6851, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6075.6685, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5718.6699, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5336.0132, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4910.1489, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4380.5625, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4008.1094, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3689.5837, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3337.9529, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3072.9187, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2790.4897, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-14 Step-68 TIME ELAPSED = 580.0571472644806
Params after OPT step  params tensor([ 0.0526, -0.0121, -0.0489,  ..., -0.0526, -0.0738,  0.6828],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2862, -0.0308,  0.6787,  1.4122,  0.4608],
        [ 0.6734, -0.5808,  0.9346,  1.4795, -0.1632],
        [-0.3667,  0.7685,  0.8249, -0.1640,  1.1410],
        [-0.4688,  0.4932,  1.3249,  1.3071,  0.8667],
        [ 1.4366,  1.0559, -0.3859,  0.7219,  0.8158]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1608, -0.8768,  0.1711],
        [ 1.9287,  0.7807, -0.7407],
        [-0.4632, -0.5264,  0.8298]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814]

 avg_word_acc_train = 0.3932045526149968
 avg_word_acc_test = 0.11163949275362318
 avg_letter_acc_train = 0.8304487140189473
 avg_letter_acc_test = 0.6317892137379031

Batch completed Epoch-1 Batch-14 Step-68 TIME ELAPSED = 649.7578115463257

----- Starting Epoch-1 Batch-15 ------
torch.float32
tensor(14019.4150, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13953.4541, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13176.5059, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12245.9854, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11319.9482, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10442.0684, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9620.4199, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8868.9688, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8183.6704, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7599.0547, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7125.4365, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6669.3145, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6231.9067, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5825.1807, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5471.9478, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5106.9668, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4782.3164, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4470.1021, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4174.6948, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3919.4241, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-15 Step-69 TIME ELAPSED = 578.2727944850922
Params after OPT step  params tensor([ 0.0535, -0.0119, -0.0246,  ..., -0.0532, -0.0751,  0.6800],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3275,  0.0364,  0.7232,  1.3469,  0.5021],
        [ 0.6705, -0.5858,  0.9411,  1.4808, -0.0718],
        [-0.3488,  0.7097,  0.7522, -0.2910,  1.1694],
        [-0.4379,  0.4662,  1.3439,  1.2967,  0.8615],
        [ 1.5195,  1.0778, -0.3610,  0.7206,  0.8030]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2488, -0.7922,  0.2736],
        [ 1.8880,  0.8369, -0.8113],
        [-0.4826, -0.5456,  0.8478]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564]

 avg_word_acc_train = 0.39584627329192545
 avg_word_acc_test = 0.11183035714285715
 avg_letter_acc_train = 0.831800892598318
 avg_letter_acc_test = 0.6313848798099324

Batch completed Epoch-1 Batch-15 Step-69 TIME ELAPSED = 647.6905903816223

----- Starting Epoch-1 Batch-16 ------
torch.float32
tensor(11239.2227, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11186.9277, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10959.8896, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10624.2207, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10224.9150, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9790.5723, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9333.0879, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8857.7061, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8369.9912, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7878.5269, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7388.5068, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6895.3262, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6391.9336, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5873.8037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5314.5479, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4755.8462, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4330.6899, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3914.3752, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3513.3826, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3131.4658, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-16 Step-70 TIME ELAPSED = 577.5975015163422
Params after OPT step  params tensor([ 0.0467, -0.0249, -0.0609,  ..., -0.0523, -0.0741,  0.6727],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3052, -0.0428,  0.6996,  1.3650,  0.5193],
        [ 0.6855, -0.5994,  0.9499,  1.4679, -0.0574],
        [-0.3390,  0.7094,  0.7814, -0.2768,  1.1842],
        [-0.4377,  0.4367,  1.3807,  1.2953,  0.9474],
        [ 1.4554,  0.9796, -0.3706,  0.6698,  0.8291]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1344, -0.8138,  0.2050],
        [ 1.9070,  0.7969, -0.7919],
        [-0.4904, -0.5282,  0.8611]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783]

 avg_word_acc_train = 0.39885372014696874
 avg_word_acc_test = 0.11245598591549295
 avg_letter_acc_train = 0.8330796707625587
 avg_letter_acc_test = 0.6316776830949119

Batch completed Epoch-1 Batch-16 Step-70 TIME ELAPSED = 646.6847920417786

----- Starting Epoch-1 Batch-17 ------
torch.float32
tensor(10976.6426, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10920.5186, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10749.7109, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10471.3281, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10109.8066, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9691.8525, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9241.1846, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8777.2100, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8316.5029, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7871.5532, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7447.7256, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7040.9307, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6640.1860, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6231.9678, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5778.5571, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5342.8521, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5001.8223, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4609.5137, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4273.2144, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3969.9973, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-17 Step-71 TIME ELAPSED = 574.8692009449005
Params after OPT step  params tensor([ 0.0623, -0.0057, -0.0678,  ..., -0.0530, -0.0748,  0.6838],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3421,  0.0283,  0.7382,  1.3324,  0.4503],
        [ 0.6966, -0.5752,  0.9664,  1.4238, -0.1092],
        [-0.3548,  0.7380,  0.8146, -0.2912,  1.1310],
        [-0.4423,  0.4693,  1.4012,  1.2829,  0.9364],
        [ 1.4495,  1.0040, -0.3771,  0.6745,  0.8047]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2393, -0.8086,  0.2901],
        [ 1.8914,  0.7955, -0.7709],
        [-0.4997, -0.5205,  0.8237]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571]

 avg_word_acc_train = 0.40025852958937197
 avg_word_acc_test = 0.11263020833333333
 avg_letter_acc_train = 0.8339067805984589
 avg_letter_acc_test = 0.6320313660678001

Batch completed Epoch-1 Batch-17 Step-71 TIME ELAPSED = 644.4283990859985

----- Starting Epoch-1 Batch-18 ------
torch.float32
tensor(10191.6299, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10130.3604, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9936.7422, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9654.1182, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9325.6797, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8983.1123, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8638.7168, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8293.2275, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7945.1060, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7595.6504, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7248.3442, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6905.4253, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6566.0410, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6226.1973, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5880.7031, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5526.7896, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5164.4248, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4784.5444, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4372.3340, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4019.6887, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-18 Step-72 TIME ELAPSED = 576.0868780612946
Params after OPT step  params tensor([ 0.0543, -0.0095, -0.0738,  ..., -0.0546, -0.0758,  0.6939],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3166,  0.0447,  0.7682,  1.3570,  0.4754],
        [ 0.7093, -0.5662,  0.9669,  1.4128, -0.1161],
        [-0.3303,  0.7370,  0.7763, -0.3060,  1.1346],
        [-0.4495,  0.5002,  1.3786,  1.3333,  0.9203],
        [ 1.4467,  1.0521, -0.3713,  0.7173,  0.7743]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2234, -0.7747,  0.2206],
        [ 1.9305,  0.8128, -0.8040],
        [-0.4872, -0.5104,  0.8535]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571]

 avg_word_acc_train = 0.4033371798689696
 avg_word_acc_test = 0.11301369863013698
 avg_letter_acc_train = 0.834970011463478
 avg_letter_acc_test = 0.632290169609894

Batch completed Epoch-1 Batch-18 Step-72 TIME ELAPSED = 645.5954346656799

----- Starting Epoch-1 Batch-19 ------
torch.float32
tensor(10060.0059, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10017.3301, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9618.4541, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9150.3301, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8689.5293, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8264.2080, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7878.2881, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7524.9902, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7194.1182, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6876.2549, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6564.2192, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6253.0337, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5939.3633, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5616.8008, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5261.9053, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4881.3579, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4604.5679, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4319.5874, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3995.2827, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3672.9380, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-19 Step-73 TIME ELAPSED = 577.4599351882935
Params after OPT step  params tensor([ 0.0487, -0.0289, -0.1251,  ..., -0.0544, -0.0761,  0.6947],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3518, -0.0019,  0.7394,  1.3780,  0.4796],
        [ 0.7453, -0.5742,  1.0194,  1.4701, -0.1272],
        [-0.3035,  0.7031,  0.7954, -0.2286,  1.1657],
        [-0.4495,  0.4390,  1.4089,  1.3304,  0.9146],
        [ 1.4505,  0.9401, -0.3828,  0.6381,  0.7611]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1903, -0.8039,  0.2290],
        [ 1.9242,  0.8016, -0.7457],
        [-0.5310, -0.4723,  0.9431]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367]

 avg_word_acc_train = 0.40527688014101054
 avg_word_acc_test = 0.11296452702702703
 avg_letter_acc_train = 0.8358851668678522
 avg_letter_acc_test = 0.6327294106623365

Batch completed Epoch-1 Batch-19 Step-73 TIME ELAPSED = 647.0989460945129

----- Starting Epoch-1 Batch-20 ------
torch.float32
tensor(11955.5352, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11897.7822, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11723.8730, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11463.5225, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11140.4941, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10777.8770, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10396.6025, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10014.6016, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9642.8193, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9283.9648, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8938.7803, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8607.9424, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8283.5371, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7950.6318, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7598.3784, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7222.6611, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6823.0913, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6398.4316, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5944.2007, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5388.2910, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-20 Step-74 TIME ELAPSED = 581.9060351848602
Params after OPT step  params tensor([ 0.0600, -0.0083, -0.0997,  ..., -0.0543, -0.0757,  0.6920],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3812,  0.0899,  0.7353,  1.2924,  0.4817],
        [ 0.7070, -0.5174,  1.0236,  1.4411, -0.1003],
        [-0.3523,  0.7102,  0.7747, -0.2519,  1.1740],
        [-0.4363,  0.4447,  1.3696,  1.3055,  0.9261],
        [ 1.4647,  0.9653, -0.4132,  0.6606,  0.8109]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2434, -0.8081,  0.2859],
        [ 1.8875,  0.7472, -0.8198],
        [-0.4951, -0.5356,  0.8167]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927]

 avg_word_acc_train = 0.40445652173913044
 avg_word_acc_test = 0.11479166666666667
 avg_letter_acc_train = 0.8357545240632374
 avg_letter_acc_test = 0.6333987095770824

Batch completed Epoch-1 Batch-20 Step-74 TIME ELAPSED = 651.7239656448364

----- Starting Epoch-1 Batch-21 ------
torch.float32
tensor(10625.2998, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10559.6943, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10366.4141, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10056.9238, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9661.5576, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9203.8330, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8701.8076, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8161.7300, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7639.4443, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7188.1914, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6757.0488, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6296.1040, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5825.0508, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5401.0649, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5026.8428, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4671.0708, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4323.3755, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4019.2444, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3684.4077, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3431.2112, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-21 Step-75 TIME ELAPSED = 582.0081763267517
Params after OPT step  params tensor([ 0.0843,  0.0356, -0.0790,  ..., -0.0533, -0.0745,  0.6964],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4244,  0.0668,  0.7500,  1.3070,  0.5056],
        [ 0.7374, -0.5460,  1.0276,  1.3706, -0.2016],
        [-0.3596,  0.6837,  0.8365, -0.2447,  1.1532],
        [-0.4021,  0.5062,  1.3913,  1.3126,  0.9297],
        [ 1.3916,  0.9961, -0.4082,  0.7203,  0.8506]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2214, -0.8608,  0.1973],
        [ 1.9273,  0.7424, -0.7543],
        [-0.4304, -0.5544,  0.9318]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158]

 avg_word_acc_train = 0.4065360411899313
 avg_word_acc_test = 0.1147203947368421
 avg_letter_acc_train = 0.8366256590665332
 avg_letter_acc_test = 0.6331676947689422

Batch completed Epoch-1 Batch-21 Step-75 TIME ELAPSED = 651.678389787674

----- Starting Epoch-1 Batch-22 ------
torch.float32
tensor(10470.7637, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10413.6953, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10173.4902, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9851.6758, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9487.2891, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9096.7783, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8689.5225, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8271.1172, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7848.4692, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7425.0425, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7002.6392, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6575.9785, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6186.4429, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5835.4663, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5474.1821, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5077.3667, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4771.5317, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4467.4863, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4150.2119, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3886.3774, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-22 Step-76 TIME ELAPSED = 582.2302334308624
Params after OPT step  params tensor([ 0.0749,  0.0195, -0.0891,  ..., -0.0538, -0.0752,  0.7034],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3786,  0.0327,  0.7515,  1.3687,  0.5443],
        [ 0.7409, -0.5485,  1.0332,  1.4298, -0.1356],
        [-0.3441,  0.7146,  0.8483, -0.2678,  1.1635],
        [-0.3841,  0.5165,  1.3763,  1.3005,  0.8742],
        [ 1.4688,  1.0531, -0.4012,  0.6672,  0.7592]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1959, -0.8022,  0.1417],
        [ 1.9544,  0.8333, -0.7798],
        [-0.4758, -0.5264,  0.9582]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197]

 avg_word_acc_train = 0.4085615471485037
 avg_word_acc_test = 0.1166801948051948
 avg_letter_acc_train = 0.8373889295921594
 avg_letter_acc_test = 0.6340802100647873

Batch completed Epoch-1 Batch-22 Step-76 TIME ELAPSED = 651.5383837223053

----- Starting Epoch-1 Batch-23 ------
torch.float32
tensor(8153.7388, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8115.4673, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7935.2705, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7662.8320, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7319.9951, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6920.1089, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6461.3154, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6003.6694, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5612.7891, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5227.1113, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4830.7554, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4487.3701, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4167.7080, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3847.6912, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3504.9802, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3259.9250, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3003.3960, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2765.9136, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2546.5022, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2274.1890, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-23 Step-77 TIME ELAPSED = 575.0153164863586
Params after OPT step  params tensor([ 0.0461, -0.0314, -0.1012,  ..., -0.0542, -0.0760,  0.7020],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2907, -0.0303,  0.7739,  1.3372,  0.5275],
        [ 0.6894, -0.5464,  1.0831,  1.4251, -0.1185],
        [-0.3661,  0.7324,  0.8397, -0.2913,  1.2010],
        [-0.3702,  0.5536,  1.4275,  1.3230,  0.8953],
        [ 1.4772,  1.1409, -0.3809,  0.5911,  0.7264]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1626, -0.7554,  0.2166],
        [ 1.9907,  0.8519, -0.7946],
        [-0.4407, -0.5892,  0.8790]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534]

 avg_word_acc_train = 0.41293896321070234
 avg_word_acc_test = 0.11678685897435898
 avg_letter_acc_train = 0.8388565068568281
 avg_letter_acc_test = 0.6343820027787471

Batch completed Epoch-1 Batch-23 Step-77 TIME ELAPSED = 644.2202706336975

----- Starting Epoch-1 Batch-24 ------
torch.float32
tensor(13964.7412, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13903.6670, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13418.0371, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12770.6133, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12083.1475, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11404.1006, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10748.5361, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10107.7412, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9476.2812, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8842.0771, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8229.8945, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7686.5693, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7201.0186, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6753.9185, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6348.6519, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5924.8350, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5502.9395, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5084.2739, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4716.0254, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4362.4160, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-24 Step-78 TIME ELAPSED = 576.4236128330231
Params after OPT step  params tensor([ 0.0673,  0.0089, -0.0693,  ..., -0.0540, -0.0752,  0.6947],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3632,  0.0084,  0.7239,  1.3741,  0.5162],
        [ 0.6916, -0.5849,  1.0336,  1.4207, -0.1926],
        [-0.3507,  0.7400,  0.8007, -0.3236,  1.1486],
        [-0.3984,  0.5488,  1.4582,  1.3246,  0.8911],
        [ 1.4747,  1.1149, -0.3614,  0.6538,  0.8171]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2229, -0.7797,  0.1830],
        [ 1.9893,  0.8134, -0.8193],
        [-0.4971, -0.5933,  0.9682]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835]

 avg_word_acc_train = 0.41305207760044027
 avg_word_acc_test = 0.11807753164556962
 avg_letter_acc_train = 0.8394367538177732
 avg_letter_acc_test = 0.6350039582311235

Batch completed Epoch-1 Batch-24 Step-78 TIME ELAPSED = 645.9968419075012

----- Starting Epoch-1 Batch-25 ------
torch.float32
tensor(10860.7480, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10789.4463, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10680.5967, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10508.9883, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10294.0566, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10046.7676, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9773.5117, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9478.6162, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9164.6973, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8832.9668, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8483.7197, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8116.5562, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7730.4653, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7322.7031, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6881.3120, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6336.7417, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5995.3086, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5632.1655, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5212.5347, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4707.5518, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-25 Step-79 TIME ELAPSED = 581.1760835647583
Params after OPT step  params tensor([ 0.0539, -0.0247, -0.0877,  ..., -0.0548, -0.0766,  0.7014],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3219,  0.0499,  0.7135,  1.3876,  0.5156],
        [ 0.6430, -0.5971,  0.9991,  1.4396, -0.1240],
        [-0.3311,  0.7921,  0.7778, -0.3869,  1.1490],
        [-0.3764,  0.5078,  1.4102,  1.3403,  0.9231],
        [ 1.5380,  1.0440, -0.4001,  0.6772,  0.7774]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2646, -0.7903,  0.1291],
        [ 1.9909,  0.8183, -0.8273],
        [-0.5183, -0.5680,  0.9301]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263]

 avg_word_acc_train = 0.4131623641304348
 avg_word_acc_test = 0.1177734375
 avg_letter_acc_train = 0.8398874702843792
 avg_letter_acc_test = 0.6348946915815172

Batch completed Epoch-1 Batch-25 Step-79 TIME ELAPSED = 650.9819629192352

----- Starting Epoch-1 Batch-26 ------
torch.float32
tensor(11947.2754, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11876.2207, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11363.7832, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10704.0781, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9953.6562, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9185.1094, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8484.2900, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7896.5620, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7379.6377, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6894.6011, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6409.9624, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5942.9482, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5527.3643, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5141.1113, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4764.5293, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4416.8149, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4055.3315, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3707.1650, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3404.4434, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3126.1926, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-26 Step-80 TIME ELAPSED = 581.6026532649994
Params after OPT step  params tensor([ 0.0504, -0.0423, -0.0934,  ..., -0.0557, -0.0793,  0.7041],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.2854,  0.0744,  0.7747,  1.4525,  0.4692],
        [ 0.6277, -0.5714,  1.0110,  1.4608, -0.1334],
        [-0.3631,  0.8527,  0.8467, -0.3999,  1.1269],
        [-0.3739,  0.4677,  1.3673,  1.3431,  0.9753],
        [ 1.5842,  1.0256, -0.4234,  0.6244,  0.7238]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1604, -0.8328,  0.1400],
        [ 2.0408,  0.8958, -0.7643],
        [-0.5394, -0.5294,  0.9455]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169]

 avg_word_acc_train = 0.4165492485238862
 avg_word_acc_test = 0.11805555555555555
 avg_letter_acc_train = 0.8412408911834214
 avg_letter_acc_test = 0.635012050770074

Batch completed Epoch-1 Batch-26 Step-80 TIME ELAPSED = 651.7026560306549

----- Starting Epoch-1 Batch-27 ------
torch.float32
tensor(15272.8506, device='cuda:0', grad_fn=<AddBackward0>)
tensor(15198.5791, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14776.4883, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14100.2559, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13303.0371, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12517.3086, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11820.4033, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11190.5596, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10584.6006, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9981.3965, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9374.6396, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8758.3975, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8092.6602, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7504.8208, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6996.9990, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6494.9097, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5996.1880, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5584.8438, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5201.9736, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4780.9888, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-27 Step-81 TIME ELAPSED = 575.435106754303
Params after OPT step  params tensor([ 0.0614,  0.0320, -0.0112,  ..., -0.0553, -0.0774,  0.7040],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4247,  0.0943,  0.7665,  1.4266,  0.5247],
        [ 0.7085, -0.6170,  0.9871,  1.4815, -0.1203],
        [-0.3271,  0.8069,  0.7796, -0.4196,  1.1714],
        [-0.4232,  0.5161,  1.3879,  1.3078,  0.9406],
        [ 1.5435,  1.1098, -0.4148,  0.6122,  0.7932]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2037, -0.8576,  0.1930],
        [ 2.0607,  0.8832, -0.8241],
        [-0.5744, -0.5995,  1.0287]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237]

 avg_word_acc_train = 0.41794803817603393
 avg_word_acc_test = 0.11775914634146341
 avg_letter_acc_train = 0.8418328261582363
 avg_letter_acc_test = 0.6350463789530075

Batch completed Epoch-1 Batch-27 Step-81 TIME ELAPSED = 644.0450797080994

----- Starting Epoch-1 Batch-28 ------
torch.float32
tensor(9103.6533, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9028.0859, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8750.0488, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8382.0938, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7968.8535, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7532.8042, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7100.5688, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6687.3257, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6319.3208, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5979.5498, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5644.4829, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5309.2300, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4966.0791, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4611.6401, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4272.2993, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3962.7568, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3657.8669, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3367.9829, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3093.0984, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2859.5999, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-28 Step-82 TIME ELAPSED = 577.136442899704
Params after OPT step  params tensor([ 0.0443,  0.0054,  0.0306,  ..., -0.0549, -0.0772,  0.7083],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3555,  0.0673,  0.7621,  1.4079,  0.5308],
        [ 0.6814, -0.6231,  0.9978,  1.5484, -0.0669],
        [-0.2890,  0.8026,  0.7894, -0.3863,  1.2418],
        [-0.3800,  0.4930,  1.3905,  1.3284,  0.9892],
        [ 1.5551,  1.0858, -0.4217,  0.5750,  0.7449]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1844, -0.8593,  0.2164],
        [ 2.0564,  0.9264, -0.8055],
        [-0.5598, -0.5978,  1.0103]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511]

 avg_word_acc_train = 0.4202543871136721
 avg_word_acc_test = 0.11803463855421686
 avg_letter_acc_train = 0.8427344386944825
 avg_letter_acc_test = 0.635248286085055

Batch completed Epoch-1 Batch-28 Step-82 TIME ELAPSED = 647.3830208778381

----- Starting Epoch-1 Batch-29 ------
torch.float32
tensor(13075.2461, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13028.9248, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12654.0820, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12163.6719, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11591.7891, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10972.0771, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10335.7090, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9705.4346, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9092.4570, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8505.1504, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7934.9971, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7377.4404, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6827.9189, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6357.2173, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5915.3740, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5481.2544, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5048.5356, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4670.4546, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4306.8589, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3932.4675, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-29 Step-83 TIME ELAPSED = 578.250524520874
Params after OPT step  params tensor([ 0.0405, -0.0310, -0.0630,  ..., -0.0547, -0.0777,  0.7137],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4208,  0.0391,  0.7082,  1.3606,  0.4943],
        [ 0.7049, -0.6536,  1.0086,  1.5195, -0.0819],
        [-0.3203,  0.8013,  0.8819, -0.3419,  1.1908],
        [-0.4254,  0.5175,  1.4852,  1.3605,  1.0050],
        [ 1.5339,  1.0656, -0.4173,  0.6086,  0.7916]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2190, -0.9712,  0.2358],
        [ 2.1561,  0.7844, -0.8302],
        [-0.4589, -0.7000,  1.0081]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862]

 avg_word_acc_train = 0.42231981107660455
 avg_word_acc_test = 0.11774553571428571
 avg_letter_acc_train = 0.8436420828271936
 avg_letter_acc_test = 0.6344335486891328

Batch completed Epoch-1 Batch-29 Step-83 TIME ELAPSED = 646.748528957367

----- Starting Epoch-1 Batch-30 ------
torch.float32
tensor(13750.1865, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13695.7510, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13394.7725, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12936.3672, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12390.0498, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11802.1797, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11188.7734, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10557.3193, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9907.2178, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9216.0283, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8672.9082, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8172.3335, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7667.2769, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7188.0137, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6727.8389, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6295.5776, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5873.9287, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5503.9316, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5138.8345, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4793.0435, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-30 Step-84 TIME ELAPSED = 578.1746399402618
Params after OPT step  params tensor([ 0.0344, -0.0050, -0.0984,  ..., -0.0554, -0.0785,  0.7290],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4941,  0.0972,  0.7128,  1.2809,  0.5550],
        [ 0.7295, -0.5731,  1.0830,  1.4514, -0.0633],
        [-0.3286,  0.7624,  0.8079, -0.4089,  1.1927],
        [-0.4092,  0.5041,  1.5047,  1.3539,  0.9291],
        [ 1.5682,  1.0496, -0.4396,  0.6312,  0.7821]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.3341, -0.8711,  0.2952],
        [ 2.1424,  0.7961, -0.8633],
        [-0.4616, -0.6144,  0.9658]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647]

 avg_word_acc_train = 0.42286604859335036
 avg_word_acc_test = 0.11727941176470588
 avg_letter_acc_train = 0.8439565332688779
 avg_letter_acc_test = 0.6335440190505756

Batch completed Epoch-1 Batch-30 Step-84 TIME ELAPSED = 647.517685174942

----- Starting Epoch-1 Batch-31 ------
torch.float32
tensor(11590.2100, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11526.3789, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11196.0879, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10757.8779, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10267.9346, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9754.6689, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9239.2461, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8736.5889, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8248.6045, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7766.7466, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7279.2148, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6776.6934, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6231.9907, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5649.9917, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5199.4956, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4763.0732, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4346.0933, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3994.1917, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3619.5134, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3243.8455, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-31 Step-85 TIME ELAPSED = 579.9197833538055
Params after OPT step  params tensor([ 0.0530, -0.0025, -0.1062,  ..., -0.0561, -0.0794,  0.7385],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4148,  0.1097,  0.7516,  1.3969,  0.5427],
        [ 0.6629, -0.6525,  1.0499,  1.4568, -0.1304],
        [-0.3245,  0.7574,  0.7687, -0.4392,  1.2199],
        [-0.3647,  0.4575,  1.4924,  1.3923,  0.9370],
        [ 1.5798,  1.0624, -0.3644,  0.7044,  0.7503]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2315, -0.8151,  0.2047],
        [ 2.1731,  0.8526, -0.8703],
        [-0.4917, -0.6015,  1.0094]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039]

 avg_word_acc_train = 0.42521644337714865
 avg_word_acc_test = 0.11736918604651163
 avg_letter_acc_train = 0.8449020936410389
 avg_letter_acc_test = 0.6336442786464294

Batch completed Epoch-1 Batch-31 Step-85 TIME ELAPSED = 649.8620820045471

----- Starting Epoch-1 Batch-32 ------
torch.float32
tensor(10950.3369, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10889.1299, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10683.1250, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10328.2207, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9883.5947, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9388.9932, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8856.1338, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8288.8906, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7706.2349, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7131.3418, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6576.4316, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6039.3877, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5514.0425, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4966.0020, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4484.9297, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4121.3525, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3754.7031, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3404.5186, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3111.5190, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2823.6582, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-32 Step-86 TIME ELAPSED = 581.2827987670898
Params after OPT step  params tensor([ 0.0283, -0.0433, -0.0765,  ..., -0.0554, -0.0780,  0.7283],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4111,  0.0583,  0.7332,  1.4249,  0.5165],
        [ 0.6830, -0.6478,  1.0265,  1.4739, -0.1466],
        [-0.2812,  0.7666,  0.7998, -0.4023,  1.2484],
        [-0.3756,  0.4886,  1.4866,  1.4001,  0.9788],
        [ 1.5647,  1.0607, -0.3979,  0.6800,  0.7886]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2020, -0.8719,  0.1967],
        [ 2.2030,  0.8407, -0.8298],
        [-0.4889, -0.6667,  1.0464]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053]

 avg_word_acc_train = 0.42841079460269865
 avg_word_acc_test = 0.11727729885057471
 avg_letter_acc_train = 0.8460273179689521
 avg_letter_acc_test = 0.6335744825293544

Batch completed Epoch-1 Batch-32 Step-86 TIME ELAPSED = 651.3878886699677

----- Starting Epoch-1 Batch-33 ------
torch.float32
tensor(9629.5645, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9581.1045, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9405.1992, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9122.9404, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8762.8506, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8357.3281, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7935.4058, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7513.7573, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7096.2856, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6683.8174, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6283.2212, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5889.3696, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5498.1089, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5160.9814, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4841.3008, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4496.2598, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4094.1531, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3773.6165, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3451.6868, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3078.1650, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-33 Step-87 TIME ELAPSED = 579.6770160198212
Params after OPT step  params tensor([ 0.0194, -0.0239, -0.0107,  ..., -0.0558, -0.0782,  0.7276],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4097,  0.0105,  0.7640,  1.4053,  0.5015],
        [ 0.7179, -0.6322,  1.0443,  1.4780, -0.1514],
        [-0.2707,  0.7259,  0.7575, -0.3424,  1.2658],
        [-0.3710,  0.4902,  1.5069,  1.4770,  0.9892],
        [ 1.5329,  1.0125, -0.4545,  0.6401,  0.7992]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1800, -0.8811,  0.1601],
        [ 2.2073,  0.8195, -0.8363],
        [-0.4907, -0.6054,  1.0617]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761]

 avg_word_acc_train = 0.4301120923913043
 avg_word_acc_test = 0.1171875
 avg_letter_acc_train = 0.8468467983661271
 avg_letter_acc_test = 0.6342749898131796

Batch completed Epoch-1 Batch-33 Step-87 TIME ELAPSED = 649.0533785820007

----- Starting Epoch-1 Batch-34 ------
torch.float32
tensor(12673.6182, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12603.3115, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12242.0605, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11717.1621, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11096.8496, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10429.4600, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9749.4746, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9077.6816, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8422.7842, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7776.8867, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7215.1050, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6726.0166, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6230.9385, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5753.3945, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5347.0869, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5000.4355, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4657.5073, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4315.6592, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4001.6338, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3666.2393, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-34 Step-88 TIME ELAPSED = 580.638665676117
Params after OPT step  params tensor([ 0.0647,  0.0113, -0.0296,  ..., -0.0556, -0.0785,  0.7321],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3632, -0.0263,  0.7484,  1.3896,  0.5601],
        [ 0.6540, -0.6910,  1.0487,  1.4689, -0.1394],
        [-0.3118,  0.7268,  0.7854, -0.3613,  1.2177],
        [-0.3866,  0.5085,  1.5116,  1.4758,  0.9959],
        [ 1.5415,  1.0409, -0.4534,  0.6519,  0.7559]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1687, -0.8209,  0.1859],
        [ 2.1791,  0.8196, -0.8511],
        [-0.5379, -0.6466,  1.0466]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451]

 avg_word_acc_train = 0.4330040913531998
 avg_word_acc_test = 0.11639747191011236
 avg_letter_acc_train = 0.8477481078196683
 avg_letter_acc_test = 0.6336695647238489

Batch completed Epoch-1 Batch-34 Step-88 TIME ELAPSED = 650.4679100513458

----- Starting Epoch-1 Batch-35 ------
torch.float32
tensor(16025.1621, device='cuda:0', grad_fn=<AddBackward0>)
tensor(15965.0439, device='cuda:0', grad_fn=<AddBackward0>)
tensor(15676.2588, device='cuda:0', grad_fn=<AddBackward0>)
tensor(15243.7871, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14718.3594, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14131.0820, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13509.1553, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12863.0527, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12192.9199, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11497.2969, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10772.9395, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10086.3623, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9459.8252, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8855.7354, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8255.5586, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7812.3291, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7323.9463, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6845.9941, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6367.3647, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5861.3496, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-35 Step-89 TIME ELAPSED = 583.7505125999451
Params after OPT step  params tensor([ 0.0608, -0.0357, -0.0999,  ..., -0.0553, -0.0790,  0.7366],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4441,  0.0624,  0.7090,  1.3894,  0.5506],
        [ 0.6515, -0.5601,  1.0056,  1.4492, -0.1906],
        [-0.3289,  0.8544,  0.7253, -0.4290,  1.2252],
        [-0.3896,  0.5366,  1.4233,  1.4157,  0.9846],
        [ 1.5881,  1.0575, -0.4264,  0.6612,  0.7955]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1877, -0.8861,  0.2042],
        [ 2.1445,  0.8134, -0.8602],
        [-0.5757, -0.6326,  0.9796]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177]

 avg_word_acc_train = 0.4314915458937198
 avg_word_acc_test = 0.11597222222222223
 avg_letter_acc_train = 0.847503601891199
 avg_letter_acc_test = 0.6341834637506636

Batch completed Epoch-1 Batch-35 Step-89 TIME ELAPSED = 654.0320355892181

----- Starting Epoch-1 Batch-36 ------
torch.float32
tensor(10871.4072, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10800.0225, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10401.2617, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9872.0869, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9276.9355, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8674.3623, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8116.7339, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7586.7588, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7068.0830, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6600.8115, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6205.2930, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5825.7666, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5448.9517, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5073.0669, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4726.8179, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4392.1411, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4076.2205, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3791.7603, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3517.6257, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3237.8594, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-36 Step-90 TIME ELAPSED = 577.9341342449188
Params after OPT step  params tensor([ 0.0428, -0.0427, -0.0427,  ..., -0.0581, -0.0795,  0.7464],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3646,  0.0253,  0.7940,  1.4669,  0.5648],
        [ 0.6392, -0.5471,  1.0268,  1.4375, -0.2048],
        [-0.3117,  0.8731,  0.7494, -0.4545,  1.2524],
        [-0.4080,  0.5183,  1.4770,  1.3689,  0.9678],
        [ 1.6199,  1.1119, -0.3702,  0.5983,  0.8228]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1075, -0.8624,  0.2261],
        [ 2.1872,  0.8478, -0.8492],
        [-0.6492, -0.6615,  1.0447]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526]

 avg_word_acc_train = 0.43396141901576685
 avg_word_acc_test = 0.11607142857142858
 avg_letter_acc_train = 0.8484758957160854
 avg_letter_acc_test = 0.634016034131657

Batch completed Epoch-1 Batch-36 Step-90 TIME ELAPSED = 647.3315870761871

----- Starting Epoch-1 Batch-37 ------
torch.float32
tensor(13646.6377, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13584.3838, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13132.9248, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12480.0498, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11745.0352, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10996.6074, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10250.1738, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9509.6592, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8780.6641, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8097.5776, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7555.9336, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7075.6567, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6661.2783, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6286.9380, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5919.3110, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5524.4033, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5108.1758, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4676.0938, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4280.4409, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3936.6228, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-37 Step-91 TIME ELAPSED = 581.2798163890839
Params after OPT step  params tensor([ 0.0281, -0.0069,  0.0149,  ..., -0.0606, -0.0786,  0.7448],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3062,  0.0215,  0.8382,  1.3968,  0.5758],
        [ 0.6637, -0.5714,  1.0894,  1.4625, -0.1677],
        [-0.3262,  0.8085,  0.7675, -0.4118,  1.2001],
        [-0.4171,  0.5542,  1.5051,  1.4148,  0.9850],
        [ 1.5643,  1.1212, -0.4405,  0.6038,  0.7863]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1721, -0.9522,  0.2401],
        [ 2.1849,  0.8006, -0.8546],
        [-0.5541, -0.6121,  1.0463]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631]

 avg_word_acc_train = 0.4353585775047259
 avg_word_acc_test = 0.11565896739130435
 avg_letter_acc_train = 0.8491862409290695
 avg_letter_acc_test = 0.6338064776965875

Batch completed Epoch-1 Batch-37 Step-91 TIME ELAPSED = 649.8683569431305

----- Starting Epoch-1 Batch-38 ------
torch.float32
tensor(10665.8662, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10598.9805, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10364.0957, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10047.2549, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9678.7568, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9278.6953, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8859.7637, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8429.4209, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7992.1895, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7548.6973, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7096.1602, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6630.0996, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6148.1279, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5636.2666, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5076.5698, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4637.3882, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4265.2222, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3911.9922, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3624.8601, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3376.5657, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-38 Step-92 TIME ELAPSED = 579.5418608188629
Params after OPT step  params tensor([ 0.0303, -0.0282, -0.0536,  ..., -0.0578, -0.0790,  0.7519],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3684,  0.0210,  0.8078,  1.3926,  0.5666],
        [ 0.6872, -0.5767,  1.0546,  1.5069, -0.1576],
        [-0.3168,  0.8449,  0.7515, -0.4170,  1.2227],
        [-0.4310,  0.5320,  1.4866,  1.4170,  0.9719],
        [ 1.5584,  1.1142, -0.4023,  0.6364,  0.7718]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1954, -0.9359,  0.2455],
        [ 2.1857,  0.8673, -0.8281],
        [-0.5483, -0.6052,  1.0837]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937]

 avg_word_acc_train = 0.4377337540906966
 avg_word_acc_test = 0.11676747311827956
 avg_letter_acc_train = 0.850107527589866
 avg_letter_acc_test = 0.6344504262556961

Batch completed Epoch-1 Batch-38 Step-92 TIME ELAPSED = 649.0125079154968

----- Starting Epoch-1 Batch-39 ------
torch.float32
tensor(10934.9180, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10877.1914, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10682.7666, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10368.9229, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9968.5322, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9527.8857, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9077.6055, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8625.1064, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8172.4189, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7722.0322, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7276.9839, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6837.1748, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6399.7070, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5950.3257, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5490.4849, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5090.7095, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4738.2344, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4396.1240, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4088.6179, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3781.7312, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-39 Step-93 TIME ELAPSED = 582.0877976417542
Params after OPT step  params tensor([ 0.0502, -0.0342, -0.0295,  ..., -0.0576, -0.0791,  0.7472],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4271,  0.0194,  0.7941,  1.4180,  0.5289],
        [ 0.7002, -0.5654,  1.0386,  1.5384, -0.1534],
        [-0.2809,  0.8558,  0.7386, -0.4195,  1.2483],
        [-0.4400,  0.5127,  1.4740,  1.4457,  1.0025],
        [ 1.5805,  1.0951, -0.4467,  0.6216,  0.7664]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1752, -0.9628,  0.1802],
        [ 2.2327,  0.8856, -0.8529],
        [-0.5908, -0.6416,  1.0809]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612]

 avg_word_acc_train = 0.4388948311748381
 avg_word_acc_test = 0.11685505319148937
 avg_letter_acc_train = 0.8507271316101758
 avg_letter_acc_test = 0.6341183883848563

Batch completed Epoch-1 Batch-39 Step-93 TIME ELAPSED = 652.1634452342987

----- Starting Epoch-1 Batch-40 ------
torch.float32
tensor(11680.1738, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11621.7305, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11236.6582, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10682.0361, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10026.6660, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9324.2461, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8614.8262, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7905.6509, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7168.8960, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6554.4194, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6082.4253, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5628.4600, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5171.8716, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4792.1958, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4450.0479, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4118.0557, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3815.7017, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3529.4211, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3246.4221, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2961.4634, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-40 Step-94 TIME ELAPSED = 586.6318771839142
Params after OPT step  params tensor([ 0.0285, -0.0374,  0.0767,  ..., -0.0581, -0.0794,  0.7489],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.3569,  0.0525,  0.8182,  1.4541,  0.5667],
        [ 0.6608, -0.5571,  1.0900,  1.4919, -0.1751],
        [-0.3124,  0.8161,  0.7938, -0.4501,  1.2305],
        [-0.4356,  0.5576,  1.6033,  1.4123,  0.9563],
        [ 1.5865,  1.0920, -0.4219,  0.5905,  0.7522]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1275, -0.8125,  0.1821],
        [ 2.2922,  0.8939, -0.8873],
        [-0.5734, -0.6493,  1.0883]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834]

 avg_word_acc_train = 0.4408538329519451
 avg_word_acc_test = 0.11677631578947369
 avg_letter_acc_train = 0.8515528109265598
 avg_letter_acc_test = 0.633933992617793

Batch completed Epoch-1 Batch-40 Step-94 TIME ELAPSED = 656.1046850681305

----- Starting Epoch-1 Batch-41 ------
torch.float32
tensor(14260.1738, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14197.9062, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13946.4209, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13559.6504, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13087.0557, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12562.2393, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12012.8730, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11452.9492, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10887.1729, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10314.8447, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9743.0039, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9182.9844, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8687.3652, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8239.5635, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7800.4126, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7343.6597, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6952.8389, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6590.9019, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6174.2017, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5860.2617, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-41 Step-95 TIME ELAPSED = 576.7251081466675
Params after OPT step  params tensor([ 0.0119, -0.0527,  0.0226,  ..., -0.0564, -0.0794,  0.7533],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4671,  0.0559,  0.7478,  1.4262,  0.5508],
        [ 0.7276, -0.5610,  1.0109,  1.4873, -0.1798],
        [-0.3016,  0.8697,  0.7827, -0.4147,  1.2216],
        [-0.4176,  0.5821,  1.5330,  1.3971,  0.9120],
        [ 1.5842,  1.0937, -0.4184,  0.6030,  0.7525]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1791, -0.8782,  0.1626],
        [ 2.2207,  0.8142, -0.8647],
        [-0.5604, -0.6581,  1.1145]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128]

 avg_word_acc_train = 0.440818897192029
 avg_word_acc_test = 0.11669921875
 avg_letter_acc_train = 0.8517846042639844
 avg_letter_acc_test = 0.6332071258837609

Batch completed Epoch-1 Batch-41 Step-95 TIME ELAPSED = 645.3658449649811

----- Starting Epoch-1 Batch-42 ------
torch.float32
tensor(9807.8213, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9755.3682, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9546.8008, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9217.6455, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8823.0381, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8404.8125, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7986.0073, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7573.0522, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7167.5444, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6768.7334, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6370.6465, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5964.4556, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5543.6880, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5071.7842, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4655.6597, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4315.9512, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3946.2502, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3652.4761, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3376.2629, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3034.4194, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-42 Step-96 TIME ELAPSED = 575.7100963592529
Params after OPT step  params tensor([ 0.0389, -0.0106, -0.0538,  ..., -0.0575, -0.0800,  0.7613],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.5022, -0.0022,  0.7450,  1.3835,  0.5585],
        [ 0.7156, -0.6505,  1.0285,  1.4792, -0.1424],
        [-0.2570,  0.8442,  0.8132, -0.4130,  1.2431],
        [-0.4259,  0.5224,  1.5306,  1.3853,  0.9586],
        [ 1.5487,  1.0662, -0.4116,  0.5380,  0.7682]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1751, -0.8220,  0.2092],
        [ 2.2271,  0.8504, -0.8439],
        [-0.5618, -0.6594,  1.0571]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087]

 avg_word_acc_train = 0.44336200134468845
 avg_word_acc_test = 0.1169458762886598
 avg_letter_acc_train = 0.8525481388347382
 avg_letter_acc_test = 0.6332681909239553

Batch completed Epoch-1 Batch-42 Step-96 TIME ELAPSED = 644.6024043560028

----- Starting Epoch-1 Batch-43 ------
torch.float32
tensor(14394.3271, device='cuda:0', grad_fn=<AddBackward0>)
tensor(14329.4355, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13951.9023, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13374.6367, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12735.7402, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12130.5430, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11587.4541, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11089.8496, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10608.4150, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10124.9971, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9637.5137, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9149.0205, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8663.7627, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8189.0830, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7733.1299, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7279.4238, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6793.1855, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6251.0190, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5637.4062, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5140.4600, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-43 Step-97 TIME ELAPSED = 581.0592021942139
Params after OPT step  params tensor([ 0.0362, -0.0093, -0.0052,  ..., -0.0579, -0.0795,  0.7503],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4260,  0.0566,  0.7881,  1.4243,  0.5479],
        [ 0.7049, -0.5884,  1.0412,  1.5220, -0.1547],
        [-0.2798,  0.8180,  0.7467, -0.4335,  1.2473],
        [-0.4356,  0.5213,  1.5302,  1.4040,  0.9682],
        [ 1.5884,  1.0711, -0.4236,  0.5963,  0.7720]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2073, -0.8634,  0.1879],
        [ 2.2266,  0.8953, -0.8828],
        [-0.6055, -0.6344,  1.0562]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724]

 avg_word_acc_train = 0.4428238686779059
 avg_word_acc_test = 0.11878188775510204
 avg_letter_acc_train = 0.8524700757770743
 avg_letter_acc_test = 0.6345472980538657

Batch completed Epoch-1 Batch-43 Step-97 TIME ELAPSED = 651.2969119548798

----- Starting Epoch-1 Batch-44 ------
torch.float32
tensor(9935.7393, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9887.0635, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9700.6406, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9416.0713, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9050.9336, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8630.3477, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8180.7607, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7721.5693, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7263.2490, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6798.5020, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6305.1138, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5864.2578, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5461.4146, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5025.4785, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4580.9419, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4226.5552, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3907.7336, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3587.4956, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3337.1436, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3089.1345, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-44 Step-98 TIME ELAPSED = 582.2829847335815
Params after OPT step  params tensor([ 0.0296,  0.0064,  0.0273,  ..., -0.0574, -0.0798,  0.7616],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4666,  0.0575,  0.7197,  1.4300,  0.5695],
        [ 0.7082, -0.6139,  0.9961,  1.5214, -0.1385],
        [-0.2437,  0.8073,  0.7421, -0.4034,  1.2703],
        [-0.4103,  0.5186,  1.5420,  1.3816,  0.8576],
        [ 1.5978,  1.0354, -0.3851,  0.7028,  0.7726]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.3245, -0.8395,  0.2910],
        [ 2.1998,  0.8946, -0.8917],
        [-0.5822, -0.6062,  0.9950]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229]

 avg_word_acc_train = 0.4449796881862099
 avg_word_acc_test = 0.1185290404040404
 avg_letter_acc_train = 0.853241982980224
 avg_letter_acc_test = 0.6340368848759743

Batch completed Epoch-1 Batch-44 Step-98 TIME ELAPSED = 652.0215268135071

----- Starting Epoch-1 Batch-45 ------
torch.float32
tensor(11831.0752, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11782.4736, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11209.6582, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10516.8770, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9803.2549, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9110.7363, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8452.3467, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7816.7930, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7205.7886, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6559.8169, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6050.7524, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5605.8384, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5134.8149, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4694.4014, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4292.4390, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3911.0408, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3548.6509, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3227.4858, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2959.8213, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2722.2681, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-45 Step-99 TIME ELAPSED = 579.2404453754425
Params after OPT step  params tensor([ 0.0339,  0.0247,  0.0585,  ..., -0.0585, -0.0804,  0.7704],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4343,  0.0340,  0.7958,  1.4639,  0.5456],
        [ 0.6690, -0.6125,  1.0771,  1.5422, -0.1714],
        [-0.2607,  0.8198,  0.7724, -0.3998,  1.2318],
        [-0.4856,  0.4893,  1.5252,  1.4563,  0.9251],
        [ 1.6035,  1.0854, -0.4506,  0.6962,  0.7729]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.1726, -0.9071,  0.1985],
        [ 2.3401,  0.8585, -0.8623],
        [-0.5497, -0.6863,  1.1146]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737]

 avg_word_acc_train = 0.4475611413043478
 avg_word_acc_test = 0.11875
 avg_letter_acc_train = 0.8542135314043899
 avg_letter_acc_test = 0.634197551223902

Batch completed Epoch-1 Batch-45 Step-99 TIME ELAPSED = 648.5588541030884

----- Starting Epoch-1 Batch-46 ------
torch.float32
tensor(12942.9316, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12889.5586, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12599.2822, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12172.7168, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11670.7998, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11116.2100, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10524.6191, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9907.7324, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9274.9629, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8670.0430, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8125.2925, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7617.7236, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7094.1387, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6561.5459, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6086.2954, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5678.8389, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5265.8125, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4937.4380, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4610.3511, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4208.4756, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-46 Step-100 TIME ELAPSED = 579.8577196598053
Params after OPT step  params tensor([ 0.0290, -0.0118, -0.0222,  ..., -0.0581, -0.0804,  0.7575],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4195,  0.0906,  0.7396,  1.4174,  0.5840],
        [ 0.6863, -0.5867,  1.0391,  1.5505, -0.1376],
        [-0.3089,  0.7887,  0.7096, -0.4118,  1.2630],
        [-0.4643,  0.5264,  1.4967,  1.4430,  0.9296],
        [ 1.5993,  1.1223, -0.4377,  0.7039,  0.7426]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.3295, -0.8695,  0.1845],
        [ 2.1921,  0.9088, -0.8578],
        [-0.6248, -0.6582,  1.1037]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125]

 avg_word_acc_train = 0.4483897438656909
 avg_word_acc_test = 0.1188118811881188
 avg_letter_acc_train = 0.854526616193826
 avg_letter_acc_test = 0.6341909246286873

Batch completed Epoch-1 Batch-46 Step-100 TIME ELAPSED = 649.3834846019745

----- Starting Epoch-1 Batch-47 ------
torch.float32
tensor(10250.1592, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10190.7793, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9932.2617, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9539.7383, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9059.4111, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8525.5000, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7974.7676, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7434.1816, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6920.2075, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6455.8262, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6026.4688, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5617.2793, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5203.5063, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4790.2012, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4361.9995, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3939.3069, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3576.8765, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3200.9622, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2911.7725, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2655.2524, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-47 Step-101 TIME ELAPSED = 580.8036770820618
Params after OPT step  params tensor([ 0.0193, -0.0650, -0.0725,  ..., -0.0586, -0.0811,  0.7719],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4635,  0.0734,  0.7555,  1.4635,  0.6094],
        [ 0.7513, -0.5795,  1.1150,  1.5841, -0.1573],
        [-0.3007,  0.7971,  0.7694, -0.4205,  1.2411],
        [-0.4740,  0.5380,  1.5705,  1.4577,  0.8839],
        [ 1.5329,  1.0878, -0.4132,  0.7025,  0.7343]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2587, -0.8838,  0.2035],
        [ 2.3228,  0.9257, -0.8223],
        [-0.5450, -0.6791,  1.1907]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278]

 avg_word_acc_train = 0.4505807757885763
 avg_word_acc_test = 0.11917892156862746
 avg_letter_acc_train = 0.8554368299669515
 avg_letter_acc_test = 0.6341145254685122

Batch completed Epoch-1 Batch-47 Step-101 TIME ELAPSED = 650.7324380874634

----- Starting Epoch-1 Batch-48 ------
torch.float32
tensor(13721.7939, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13662.9570, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13314.1465, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12807.2666, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12211.6650, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11562.8135, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10881.5381, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10187.1279, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9494.1406, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8790.7520, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8118.8921, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7635.8818, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7203.7192, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6736.3516, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6267.9565, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5793.3672, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5301.8613, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4804.7554, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4370.0234, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4009.4897, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-48 Step-102 TIME ELAPSED = 580.1398084163666
Params after OPT step  params tensor([ 0.0254, -0.0378, -0.0121,  ..., -0.0583, -0.0809,  0.7777],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4615,  0.1383,  0.8154,  1.4822,  0.5402],
        [ 0.7115, -0.5279,  1.0771,  1.5468, -0.2147],
        [-0.2444,  0.8919,  0.7712, -0.4667,  1.1599],
        [-0.4812,  0.5514,  1.5708,  1.4706,  0.9598],
        [ 1.6461,  1.1061, -0.4416,  0.6654,  0.7804]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2528, -0.9573,  0.1620],
        [ 2.3416,  0.9122, -0.8547],
        [-0.6584, -0.7152,  1.1256]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441]

 avg_word_acc_train = 0.4516673701983959
 avg_word_acc_test = 0.11878033980582524
 avg_letter_acc_train = 0.8557616072380382
 avg_letter_acc_test = 0.6334049050469465

Batch completed Epoch-1 Batch-48 Step-102 TIME ELAPSED = 649.8081998825073

----- Starting Epoch-1 Batch-49 ------
torch.float32
tensor(13630.7227, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13546.3457, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13181.8945, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12676.1250, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12073.1084, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11395.7881, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10686.9277, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10020.1514, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9402.7832, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8807.0420, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8222.4219, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7675.8760, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7151.5518, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6639.1201, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6130.4443, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5686.8364, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5270.6060, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4852.3110, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4486.7510, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4140.4038, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-49 Step-103 TIME ELAPSED = 581.4518065452576
Params after OPT step  params tensor([ 0.0365, -0.0310,  0.0047,  ..., -0.0584, -0.0815,  0.7770],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4213,  0.1689,  0.8307,  1.4817,  0.5886],
        [ 0.6558, -0.5425,  1.0741,  1.5499, -0.1601],
        [-0.2516,  0.9090,  0.7196, -0.5118,  1.2428],
        [-0.4912,  0.5933,  1.5832,  1.3889,  0.9728],
        [ 1.6488,  1.1369, -0.4017,  0.6276,  0.7932]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2605, -0.9152,  0.1606],
        [ 2.3149,  0.9233, -0.8711],
        [-0.7005, -0.6572,  1.1748]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633]

 avg_word_acc_train = 0.4536345108695652
 avg_word_acc_test = 0.11838942307692307
 avg_letter_acc_train = 0.856385979523746
 avg_letter_acc_test = 0.6334001597330823

Batch completed Epoch-1 Batch-49 Step-103 TIME ELAPSED = 650.7113420963287

----- Starting Epoch-1 Batch-50 ------
torch.float32
tensor(13846.9580, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13789.7793, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13525.2842, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13057.6875, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12459.8203, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11785.3477, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11067.3906, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10325.9121, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9564.6982, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8766.0752, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8098.9312, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7510.4097, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6915.4155, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6307.3638, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5903.1484, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5511.3223, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5146.1724, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4800.6694, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4434.9199, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4111.2695, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-50 Step-104 TIME ELAPSED = 584.4008383750916
Params after OPT step  params tensor([ 0.0320, -0.0473, -0.0195,  ..., -0.0592, -0.0809,  0.7758],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4342,  0.0360,  0.7321,  1.4833,  0.5594],
        [ 0.7096, -0.6015,  1.0576,  1.6021, -0.1920],
        [-0.2258,  0.8544,  0.6777, -0.4326,  1.2651],
        [-0.4679,  0.5493,  1.5677,  1.5183,  0.9617],
        [ 1.6608,  1.1308, -0.4362,  0.7142,  0.7026]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.3681, -0.9239,  0.2360],
        [ 2.3760,  0.8938, -0.9037],
        [-0.5983, -0.6874,  1.1315]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961]

 avg_word_acc_train = 0.45467132505175983
 avg_word_acc_test = 0.11830357142857142
 avg_letter_acc_train = 0.8568655990687202
 avg_letter_acc_test = 0.6335127693693383

Batch completed Epoch-1 Batch-50 Step-104 TIME ELAPSED = 653.7257945537567

----- Starting Epoch-1 Batch-51 ------
torch.float32
tensor(11678.1855, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11612.6543, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11336.0127, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10903.9277, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10395.9033, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9847.2812, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9280.4580, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8709.1660, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8141.6367, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7591.9985, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7057.8022, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6533.7070, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6012.7329, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5489.9282, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4945.7173, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4412.4995, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4008.7729, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3613.3979, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3239.7156, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2935.8694, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-51 Step-105 TIME ELAPSED = 577.1934342384338
Params after OPT step  params tensor([ 0.0505,  0.0069, -0.0291,  ..., -0.0599, -0.0814,  0.7859],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4581,  0.1149,  0.7806,  1.4623,  0.5864],
        [ 0.7108, -0.5918,  1.0295,  1.5300, -0.1787],
        [-0.2303,  0.8963,  0.7112, -0.5353,  1.2710],
        [-0.4702,  0.5781,  1.5933,  1.4570,  1.0092],
        [ 1.6388,  1.1094, -0.4384,  0.6839,  0.8058]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2441, -0.9007,  0.2013],
        [ 2.3960,  0.9004, -0.9190],
        [-0.6378, -0.6865,  1.1762]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533]

 avg_word_acc_train = 0.45672041632485644
 avg_word_acc_test = 0.1179245283018868
 avg_letter_acc_train = 0.8576198780597067
 avg_letter_acc_test = 0.6333889739557733

Batch completed Epoch-1 Batch-51 Step-105 TIME ELAPSED = 646.9326298236847

----- Starting Epoch-1 Batch-52 ------
torch.float32
tensor(10320.2607, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10269.6992, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10004.0664, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9617.8711, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9159.0186, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8664.3623, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8156.2339, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7654.7490, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7168.9028, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6696.7710, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6235.5903, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5779.8872, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5321.3247, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4838.6592, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4307.0278, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3854.0073, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3511.2007, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3153.0146, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2857.0444, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2607.1982, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-52 Step-106 TIME ELAPSED = 578.0778639316559
Params after OPT step  params tensor([ 0.0424,  0.0409,  0.0276,  ..., -0.0600, -0.0811,  0.7779],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4685,  0.0946,  0.7975,  1.4633,  0.5876],
        [ 0.7109, -0.6124,  1.0679,  1.5273, -0.1530],
        [-0.2270,  0.8632,  0.7102, -0.5484,  1.2901],
        [-0.4419,  0.5567,  1.5631,  1.4054,  1.0122],
        [ 1.6694,  1.1402, -0.4383,  0.6474,  0.8334]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2303, -0.8539,  0.1971],
        [ 2.4213,  0.9028, -0.9018],
        [-0.6019, -0.7100,  1.1740]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225]

 avg_word_acc_train = 0.45916929093864284
 avg_word_acc_test = 0.11769859813084112
 avg_letter_acc_train = 0.8584448086001529
 avg_letter_acc_test = 0.6337129877983811

Batch completed Epoch-1 Batch-52 Step-106 TIME ELAPSED = 647.1007659435272

----- Starting Epoch-1 Batch-53 ------
torch.float32
tensor(10737.1836, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10662.6416, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10487.7266, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10190.9199, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9791.8330, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9319.3750, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8802.3340, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8264.8857, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7715.1416, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7165.3130, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6625.8848, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6110.0864, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5623.6353, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5152.4736, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4697.7715, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4253.2876, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3840.6289, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3441.3091, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3145.2283, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2869.3115, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-1 Batch-53 Step-107 TIME ELAPSED = 416.1358082294464
Params after OPT step  params tensor([ 0.0250,  0.0191, -0.0115,  ..., -0.0592, -0.0821,  0.7768],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4989,  0.0890,  0.7832,  1.4234,  0.5895],
        [ 0.7427, -0.6215,  1.0761,  1.5058, -0.1644],
        [-0.2447,  0.8338,  0.7225, -0.5784,  1.2922],
        [-0.4497,  0.5531,  1.5810,  1.4047,  1.0256],
        [ 1.6338,  1.1561, -0.3908,  0.6559,  0.8354]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2517, -0.8063,  0.1762],
        [ 2.4024,  0.9682, -0.9064],
        [-0.5476, -0.6717,  1.1934]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333]

 avg_word_acc_train = 0.4615602355072464
 avg_word_acc_test = 0.11733217592592593
 avg_letter_acc_train = 0.8591633428874469
 avg_letter_acc_test = 0.6337094724792605

Batch completed Epoch-1 Batch-53 Step-107 TIME ELAPSED = 475.62125062942505
Epoch completed Epoch-1 Batch-53 Step-107 TIME ELAPSED = 35316.92524790764

--------------Starting Epoch 2-------------------
Loaded dataset... 

----- Starting Epoch-2 Batch-0 ------
torch.float32
tensor(13902.2354, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13810.4951, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13402.3096, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12842.7881, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12222.6172, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11586.8232, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10923.2617, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10243.0986, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9580.2803, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8942.4561, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8339.6514, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7777.3696, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7255.9106, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6765.1807, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6293.3037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5900.6328, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5519.7495, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5176.5479, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4850.6997, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4529.6655, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-0 Step-108 TIME ELAPSED = 577.7397630214691
Params after OPT step  params tensor([ 0.0331, -0.0118,  0.0194,  ..., -0.0596, -0.0816,  0.7809],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4913,  0.1657,  0.8134,  1.4538,  0.6070],
        [ 0.7403, -0.5449,  1.1289,  1.5053, -0.2034],
        [-0.1934,  0.8730,  0.7298, -0.5761,  1.2167],
        [-0.4682,  0.5590,  1.6155,  1.4425,  0.9983],
        [ 1.6332,  1.1210, -0.4043,  0.7119,  0.7908]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.3257, -0.9109,  0.1857],
        [ 2.4034,  0.9453, -0.8598],
        [-0.5703, -0.7109,  1.1950]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322]

 avg_word_acc_train = 0.46176954527323494
 avg_word_acc_test = 0.1172591743119266
 avg_letter_acc_train = 0.8592704818268267
 avg_letter_acc_test = 0.633342306336507

Batch completed Epoch-2 Batch-0 Step-108 TIME ELAPSED = 647.8180491924286

----- Starting Epoch-2 Batch-1 ------
torch.float32
tensor(10998.0010, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10938.6436, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10719.6865, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10408.2295, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10034.0811, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9625.9023, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9206.0645, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8790.3320, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8390.7715, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8011.9160, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7637.1035, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7247.7681, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6835.0005, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6388.6348, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5868.5142, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5366.1094, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4962.9102, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4533.0137, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4127.3223, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3780.8098, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-1 Step-109 TIME ELAPSED = 579.8724615573883
Params after OPT step  params tensor([ 0.0283,  0.0096,  0.0377,  ..., -0.0609, -0.0817,  0.7830],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4632,  0.1346,  0.8327,  1.4776,  0.5748],
        [ 0.6579, -0.6167,  1.0868,  1.5242, -0.2207],
        [-0.2289,  0.9028,  0.7474, -0.5242,  1.2210],
        [-0.4864,  0.5581,  1.6025,  1.4204,  0.9489],
        [ 1.6497,  1.1712, -0.3898,  0.6632,  0.7795]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.3205, -0.8818,  0.1956],
        [ 2.3660,  0.9668, -0.8825],
        [-0.6151, -0.6996,  1.1449]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306]

 avg_word_acc_train = 0.46254323122529645
 avg_word_acc_test = 0.1171875
 avg_letter_acc_train = 0.859568663670051
 avg_letter_acc_test = 0.6330931235824463

Batch completed Epoch-2 Batch-1 Step-109 TIME ELAPSED = 648.8247675895691

----- Starting Epoch-2 Batch-2 ------
torch.float32
tensor(13910.9707, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13844.1582, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13448.3779, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12870.0449, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12169.2061, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11391.6484, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10558.9912, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9921.6650, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9364.8428, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8794.9678, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8269.5742, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7810.1724, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7333.7021, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6829.6265, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6326.8721, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5888.5308, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5457.3921, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4999.0439, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4571.0142, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4174.6919, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-2 Step-110 TIME ELAPSED = 582.3358175754547
Params after OPT step  params tensor([ 0.0058, -0.0569,  0.0290,  ..., -0.0597, -0.0831,  0.7893],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4643,  0.1191,  0.7725,  1.4880,  0.5537],
        [ 0.6913, -0.5570,  1.0711,  1.5281, -0.1947],
        [-0.2401,  0.9598,  0.7376, -0.5888,  1.2937],
        [-0.4689,  0.6015,  1.6230,  1.4653,  0.9864],
        [ 1.6522,  1.1488, -0.4500,  0.6456,  0.8321]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2424, -0.8784,  0.1113],
        [ 2.4521,  0.9965, -0.9343],
        [-0.6365, -0.7091,  1.2075]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983]

 avg_word_acc_train = 0.4628806795926361
 avg_word_acc_test = 0.11683558558558559
 avg_letter_acc_train = 0.8600132877188389
 avg_letter_acc_test = 0.632991822824909

Batch completed Epoch-2 Batch-2 Step-110 TIME ELAPSED = 651.6916496753693

----- Starting Epoch-2 Batch-3 ------
torch.float32
tensor(11375.9727, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11307.6260, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11002.8574, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10549.3916, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10010.7578, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9431.8496, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8847.6006, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8283.7979, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7753.9927, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7255.9800, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6749.7681, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6277.3853, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5877.2373, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5488.0728, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5123.3936, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4760.1475, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4403.6108, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4068.6936, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3771.1741, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3492.1201, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-3 Step-111 TIME ELAPSED = 584.7484986782074
Params after OPT step  params tensor([ 0.0118, -0.0803,  0.0257,  ..., -0.0594, -0.0827,  0.7963],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4639,  0.1322,  0.7896,  1.5487,  0.5579],
        [ 0.7120, -0.5273,  1.0957,  1.6060, -0.1700],
        [-0.2043,  0.9387,  0.6727, -0.5742,  1.2767],
        [-0.4382,  0.6291,  1.6006,  1.4563,  0.9794],
        [ 1.6722,  1.1590, -0.4681,  0.6291,  0.8009]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2805, -0.8811,  0.0887],
        [ 2.4114,  1.0127, -0.8857],
        [-0.7455, -0.7065,  1.3202]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888]

 avg_word_acc_train = 0.46390964673913043
 avg_word_acc_test = 0.11732700892857142
 avg_letter_acc_train = 0.8603682780924851
 avg_letter_acc_test = 0.6325734114953561

Batch completed Epoch-2 Batch-3 Step-111 TIME ELAPSED = 656.4948709011078

----- Starting Epoch-2 Batch-4 ------
torch.float32
tensor(13041.0830, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12967.0879, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12724.9609, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12307.5352, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11785.7148, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11219.1699, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10645.2588, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10066.8398, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9482.4082, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8905.5791, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8353.8906, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7826.4287, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7303.5342, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6770.1401, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6218.0635, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5585.5840, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5115.1367, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4717.8398, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4261.6040, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3900.7590, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-4 Step-112 TIME ELAPSED = 586.5441403388977
Params after OPT step  params tensor([ 0.0233, -0.0091,  0.0651,  ..., -0.0600, -0.0828,  0.7968],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4961,  0.1077,  0.8591,  1.5480,  0.5565],
        [ 0.7314, -0.6262,  1.1149,  1.6153, -0.1815],
        [-0.2050,  0.8588,  0.7075, -0.5493,  1.2559],
        [-0.4564,  0.5619,  1.6262,  1.4639,  0.9757],
        [ 1.6709,  1.1465, -0.4387,  0.6354,  0.7652]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2519, -0.9009,  0.0828],
        [ 2.4819,  1.0007, -0.9265],
        [-0.6476, -0.7144,  1.2632]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604]

 avg_word_acc_train = 0.46436730473258947
 avg_word_acc_test = 0.11767146017699115
 avg_letter_acc_train = 0.8607602872633624
 avg_letter_acc_test = 0.632497248049811

Batch completed Epoch-2 Batch-4 Step-112 TIME ELAPSED = 660.1392574310303

----- Starting Epoch-2 Batch-5 ------
torch.float32
tensor(12257.4287, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12193.8457, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11762.6826, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11218.6807, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10630.7119, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10029.8027, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9437.4492, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8872.8037, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8378.9102, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7931.6123, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7488.1836, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7041.6558, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6582.7925, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6141.5591, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5711.4136, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5297.1406, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4871.9976, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4470.6987, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4101.0972, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3749.8606, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-5 Step-113 TIME ELAPSED = 584.5677828788757
Params after OPT step  params tensor([ 0.0435,  0.0212,  0.0496,  ..., -0.0595, -0.0831,  0.7875],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4734,  0.1529,  0.8877,  1.5405,  0.5641],
        [ 0.6944, -0.6140,  1.1509,  1.6100, -0.2101],
        [-0.2210,  0.9121,  0.7594, -0.5543,  1.2739],
        [-0.4748,  0.5603,  1.6088,  1.4392,  0.9355],
        [ 1.6633,  1.1641, -0.4424,  0.6241,  0.7763]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2626, -0.9211,  0.1211],
        [ 2.4652,  0.9974, -0.9044],
        [-0.6723, -0.7455,  1.2832]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073]

 avg_word_acc_train = 0.4652281178489703
 avg_word_acc_test = 0.11746162280701754
 avg_letter_acc_train = 0.8612343838858876
 avg_letter_acc_test = 0.6323125339634886

Batch completed Epoch-2 Batch-5 Step-113 TIME ELAPSED = 654.69677901268

----- Starting Epoch-2 Batch-6 ------
torch.float32
tensor(13597.4072, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13513.5000, device='cuda:0', grad_fn=<AddBackward0>)
tensor(13235.5234, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12816.2793, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12295.7725, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11737.5498, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11213.6934, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10751.1084, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10300.6973, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9844.7705, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9387.5830, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8927.7705, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8458.2119, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7968.4160, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7361.2104, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6950.4810, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6552.4673, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6071.2554, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5610.6284, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5246.4038, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-6 Step-114 TIME ELAPSED = 577.9429531097412
Params after OPT step  params tensor([ 0.0276, -0.0264,  0.0309,  ..., -0.0603, -0.0827,  0.7902],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4796,  0.1443,  0.8194,  1.4908,  0.5837],
        [ 0.7098, -0.6037,  1.1256,  1.6064, -0.2004],
        [-0.2057,  0.9117,  0.7479, -0.5494,  1.2483],
        [-0.4508,  0.5603,  1.5772,  1.4543,  0.9443],
        [ 1.6567,  1.1626, -0.4308,  0.6651,  0.7750]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.3258, -0.9061,  0.1420],
        [ 2.4011,  1.0018, -0.9245],
        [-0.6492, -0.7114,  1.2348]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625, 0.3125]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373, 0.8574423480083857]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375, 0.171875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073, 0.6514522821576764]

 avg_word_acc_train = 0.4639000472589792
 avg_word_acc_test = 0.11793478260869565
 avg_letter_acc_train = 0.8612014096608659
 avg_letter_acc_test = 0.6324789665564816

Batch completed Epoch-2 Batch-6 Step-114 TIME ELAPSED = 649.5709645748138

----- Starting Epoch-2 Batch-7 ------
torch.float32
tensor(8693.2246, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8633.3564, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8395.2500, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8064.1812, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7674.8428, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7251.6792, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6813.9355, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6372.0806, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5929.4146, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5488.9839, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5070.5293, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4703.8130, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4368.9468, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4040.1228, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3710.2649, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3437.9573, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3169.6858, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2900.5610, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2662.0442, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2424.4141, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-7 Step-115 TIME ELAPSED = 588.091739654541
Params after OPT step  params tensor([ 0.0116, -0.0528,  0.0867,  ..., -0.0617, -0.0826,  0.8025],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4750,  0.1116,  0.8566,  1.4974,  0.5932],
        [ 0.7034, -0.6369,  1.1174,  1.6073, -0.1733],
        [-0.2601,  0.8717,  0.7314, -0.5698,  1.2252],
        [-0.4662,  0.5348,  1.5720,  1.4655,  0.9615],
        [ 1.6719,  1.1664, -0.4632,  0.7008,  0.7898]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2627, -0.8910,  0.1593],
        [ 2.4640,  1.0279, -0.9438],
        [-0.5955, -0.6894,  1.1980]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625, 0.3125, 0.6875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373, 0.8574423480083857, 0.9451219512195121]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375, 0.171875, 0.15625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073, 0.6514522821576764, 0.6607142857142857]

 avg_word_acc_train = 0.46582763305847075
 avg_word_acc_test = 0.11826508620689655
 avg_letter_acc_train = 0.861924862605337
 avg_letter_acc_test = 0.6327223744802558

Batch completed Epoch-2 Batch-7 Step-115 TIME ELAPSED = 657.9936382770538

----- Starting Epoch-2 Batch-8 ------
torch.float32
tensor(12126.2588, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12069.9033, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11821.8428, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11420.0234, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10950.9873, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10486.4199, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10037.9014, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9593.8252, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9156.0547, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8725.2568, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8296.5430, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7863.8472, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7427.3262, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6992.0181, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6559.2417, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6123.1284, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5666.0269, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5242.8916, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4931.9424, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4584.9160, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-8 Step-116 TIME ELAPSED = 582.0571494102478
Params after OPT step  params tensor([ 0.0235, -0.0423,  0.0375,  ..., -0.0610, -0.0827,  0.7986],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.5100,  0.1312,  0.8577,  1.4849,  0.5855],
        [ 0.7084, -0.6025,  1.1408,  1.5805, -0.1947],
        [-0.2306,  0.9069,  0.7307, -0.5780,  1.2248],
        [-0.4578,  0.5544,  1.6031,  1.4647,  0.9586],
        [ 1.7203,  1.1589, -0.4850,  0.7043,  0.7854]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2581, -0.9733,  0.1551],
        [ 2.4887,  0.9501, -0.9163],
        [-0.6119, -0.7182,  1.2856]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625, 0.3125, 0.6875, 0.40625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373, 0.8574423480083857, 0.9451219512195121, 0.8775100401606426]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375, 0.171875, 0.15625, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073, 0.6514522821576764, 0.6607142857142857, 0.6647398843930635]

 avg_word_acc_train = 0.4653184225195095
 avg_word_acc_test = 0.11805555555555555
 avg_letter_acc_train = 0.8620580692511088
 avg_letter_acc_test = 0.6329960284111344

Batch completed Epoch-2 Batch-8 Step-116 TIME ELAPSED = 652.2434449195862

----- Starting Epoch-2 Batch-9 ------
torch.float32
tensor(9201.5889, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9148.1045, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8973.4053, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8738.7275, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8466.9883, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8165.9033, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7840.9146, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7497.5444, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7140.4873, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6769.4854, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6378.7529, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5965.1426, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5531.7114, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5087.1909, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4644.5884, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4169.7822, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3779.1091, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3474.7837, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3116.6428, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2896.8269, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-9 Step-117 TIME ELAPSED = 585.8989932537079
Params after OPT step  params tensor([ 0.0151, -0.0456,  0.0319,  ..., -0.0606, -0.0831,  0.8099],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.5549,  0.1063,  0.8306,  1.4596,  0.6321],
        [ 0.7506, -0.5923,  1.1092,  1.5464, -0.1995],
        [-0.2065,  0.9256,  0.6988, -0.5593,  1.2235],
        [-0.4399,  0.5770,  1.5749,  1.5114,  0.9685],
        [ 1.7220,  1.1512, -0.5293,  0.7048,  0.7717]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2200, -0.9654,  0.1926],
        [ 2.4940,  0.9288, -0.8838],
        [-0.6562, -0.7106,  1.2919]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625, 0.3125, 0.6875, 0.40625, 0.671875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373, 0.8574423480083857, 0.9451219512195121, 0.8775100401606426, 0.9398797595190381]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375, 0.171875, 0.15625, 0.09375, 0.0625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073, 0.6514522821576764, 0.6607142857142857, 0.6647398843930635, 0.5755693581780539]

 avg_word_acc_train = 0.46706890198968315
 avg_word_acc_test = 0.11758474576271187
 avg_letter_acc_train = 0.862717575100837
 avg_letter_acc_test = 0.632509361714244

Batch completed Epoch-2 Batch-9 Step-117 TIME ELAPSED = 656.4363100528717

----- Starting Epoch-2 Batch-10 ------
torch.float32
tensor(12531.5332, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12468.9492, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12174.5361, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11745.9414, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11248.0977, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10725.8145, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10214.2109, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9721.1396, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9237.6104, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8763.4834, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8272.0410, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7811.0581, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7392.6255, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6988.6729, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6595.5840, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6177.3779, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5743.1387, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5282.2227, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4875.7090, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4468.6963, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-10 Step-118 TIME ELAPSED = 602.4870414733887
Params after OPT step  params tensor([ 0.0059, -0.0497,  0.0471,  ..., -0.0611, -0.0834,  0.8138],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.5203,  0.1031,  0.7966,  1.5029,  0.6507],
        [ 0.7462, -0.6262,  1.0963,  1.5862, -0.1945],
        [-0.2019,  0.9157,  0.7444, -0.5491,  1.2655],
        [-0.4193,  0.5567,  1.5866,  1.4868,  0.9650],
        [ 1.7207,  1.1384, -0.4972,  0.6473,  0.7860]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2548, -0.8945,  0.1938],
        [ 2.5002,  1.0088, -0.8716],
        [-0.6648, -0.7543,  1.2640]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625, 0.3125, 0.6875, 0.40625, 0.671875, 0.421875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373, 0.8574423480083857, 0.9451219512195121, 0.8775100401606426, 0.9398797595190381, 0.8721973094170403]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375, 0.171875, 0.15625, 0.09375, 0.0625, 0.140625]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073, 0.6514522821576764, 0.6607142857142857, 0.6647398843930635, 0.5755693581780539, 0.6889352818371608]

 avg_word_acc_train = 0.4666891213006942
 avg_word_acc_test = 0.11777836134453781
 avg_letter_acc_train = 0.8627972367337464
 avg_letter_acc_test = 0.6329835291102348

Batch completed Epoch-2 Batch-10 Step-118 TIME ELAPSED = 676.6281497478485

----- Starting Epoch-2 Batch-11 ------
torch.float32
tensor(10127.8662, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10075.9277, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9899.7012, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9632.4883, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9303.7070, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8939.6016, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8561.6787, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8179.9131, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7790.1582, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7385.0850, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6961.6792, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6521.4766, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6068.1567, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5593.4512, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5059.0640, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4583.0254, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4236.1479, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3892.7075, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3639.5894, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3374.4543, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-11 Step-119 TIME ELAPSED = 618.2567458152771
Params after OPT step  params tensor([ 0.0260, -0.0421,  0.0478,  ..., -0.0608, -0.0832,  0.8091],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4755,  0.1287,  0.8078,  1.5134,  0.6740],
        [ 0.7176, -0.6090,  1.0902,  1.5867, -0.1643],
        [-0.2050,  0.9359,  0.7132, -0.5428,  1.2774],
        [-0.4622,  0.5659,  1.5698,  1.4829,  0.9788],
        [ 1.7239,  1.1342, -0.5396,  0.6599,  0.7808]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2753, -0.8600,  0.1559],
        [ 2.4780,  1.0283, -0.9287],
        [-0.6915, -0.7489,  1.2480]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625, 0.3125, 0.6875, 0.40625, 0.671875, 0.421875, 0.5625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373, 0.8574423480083857, 0.9451219512195121, 0.8775100401606426, 0.9398797595190381, 0.8721973094170403, 0.9024390243902439]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375, 0.171875, 0.15625, 0.09375, 0.0625, 0.140625, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073, 0.6514522821576764, 0.6607142857142857, 0.6647398843930635, 0.5755693581780539, 0.6889352818371608, 0.6244725738396625]

 avg_word_acc_train = 0.4674875452898551
 avg_word_acc_test = 0.117578125
 avg_letter_acc_train = 0.8631275849642173
 avg_letter_acc_test = 0.6329126044829801

Batch completed Epoch-2 Batch-11 Step-119 TIME ELAPSED = 693.8256072998047

----- Starting Epoch-2 Batch-12 ------
torch.float32
tensor(11084.0547, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11031.7676, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10721.8145, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10264.6621, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9739.2256, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9185.9805, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8623.0723, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8056.0684, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7486.6841, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6917.5059, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6363.1206, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5772.6533, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5321.5493, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4939.0215, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4546.6401, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4086.7207, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3716.4319, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3410.5994, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3075.4465, device='cuda:0', grad_fn=<AddBackward0>)
tensor(2845.7419, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-12 Step-120 TIME ELAPSED = 621.0648596286774
Params after OPT step  params tensor([ 0.0431, -0.0274, -0.0085,  ..., -0.0609, -0.0831,  0.8016],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4684,  0.1008,  0.8519,  1.4577,  0.6147],
        [ 0.7040, -0.6133,  1.1034,  1.5458, -0.2025],
        [-0.2253,  0.9181,  0.7240, -0.5192,  1.2497],
        [-0.4710,  0.5728,  1.5939,  1.5329,  0.9747],
        [ 1.6639,  1.1531, -0.5009,  0.7441,  0.7721]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2288, -0.9399,  0.1872],
        [ 2.4592,  0.9388, -0.9377],
        [-0.6258, -0.7101,  1.2575]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625, 0.3125, 0.6875, 0.40625, 0.671875, 0.421875, 0.5625, 0.671875]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373, 0.8574423480083857, 0.9451219512195121, 0.8775100401606426, 0.9398797595190381, 0.8721973094170403, 0.9024390243902439, 0.9424778761061947]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375, 0.171875, 0.15625, 0.09375, 0.0625, 0.140625, 0.09375, 0.1875]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073, 0.6514522821576764, 0.6607142857142857, 0.6647398843930635, 0.5755693581780539, 0.6889352818371608, 0.6244725738396625, 0.6457399103139013]

 avg_word_acc_train = 0.4691766978081207
 avg_word_acc_test = 0.11815599173553719
 avg_letter_acc_train = 0.8637833724943162
 avg_letter_acc_test = 0.6330186152749713

Batch completed Epoch-2 Batch-12 Step-120 TIME ELAPSED = 696.2395875453949

----- Starting Epoch-2 Batch-13 ------
torch.float32
tensor(11502.9385, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11458.2871, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11228.7607, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10871.5713, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10421.7432, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9913.1123, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9372.9092, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8821.1553, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8273.7627, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7734.6064, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7195.0977, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6635.9180, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6049.3428, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5602.4043, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5176.2788, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4762.9961, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4386.0234, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4005.5288, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3652.9819, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3315.6362, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-13 Step-121 TIME ELAPSED = 617.4154233932495
Params after OPT step  params tensor([ 0.0207, -0.0595, -0.0547,  ..., -0.0611, -0.0839,  0.8116],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.4999,  0.1305,  0.8672,  1.4712,  0.6334],
        [ 0.7085, -0.6368,  1.1050,  1.5900, -0.2177],
        [-0.2466,  0.9193,  0.7019, -0.5234,  1.2453],
        [-0.4685,  0.5967,  1.5912,  1.5189,  0.9371],
        [ 1.7101,  1.1724, -0.5128,  0.6919,  0.7638]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2904, -0.9851,  0.2402],
        [ 2.4944,  0.9374, -0.9036],
        [-0.6949, -0.7783,  1.2256]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625, 0.3125, 0.6875, 0.40625, 0.671875, 0.421875, 0.5625, 0.671875, 0.59375]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373, 0.8574423480083857, 0.9451219512195121, 0.8775100401606426, 0.9398797595190381, 0.8721973094170403, 0.9024390243902439, 0.9424778761061947, 0.9126637554585153]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375, 0.171875, 0.15625, 0.09375, 0.0625, 0.140625, 0.09375, 0.1875, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073, 0.6514522821576764, 0.6607142857142857, 0.6647398843930635, 0.5755693581780539, 0.6889352818371608, 0.6244725738396625, 0.6457399103139013, 0.6282306163021869]

 avg_word_acc_train = 0.47019779044903776
 avg_word_acc_test = 0.11795594262295082
 avg_letter_acc_train = 0.8641840313710719
 avg_letter_acc_test = 0.6329793693817517

Batch completed Epoch-2 Batch-13 Step-121 TIME ELAPSED = 692.590874671936

----- Starting Epoch-2 Batch-14 ------
torch.float32
tensor(12394.9404, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12333.5889, device='cuda:0', grad_fn=<AddBackward0>)
tensor(12012.8438, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11557.9199, device='cuda:0', grad_fn=<AddBackward0>)
tensor(11027.8848, device='cuda:0', grad_fn=<AddBackward0>)
tensor(10464.3945, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9895.7246, device='cuda:0', grad_fn=<AddBackward0>)
tensor(9344.2988, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8812.2051, device='cuda:0', grad_fn=<AddBackward0>)
tensor(8287.1611, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7747.0737, device='cuda:0', grad_fn=<AddBackward0>)
tensor(7200.6812, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6705.4858, device='cuda:0', grad_fn=<AddBackward0>)
tensor(6220.9028, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5736.9600, device='cuda:0', grad_fn=<AddBackward0>)
tensor(5298.4272, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4883.9224, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4491.1211, device='cuda:0', grad_fn=<AddBackward0>)
tensor(4154.3867, device='cuda:0', grad_fn=<AddBackward0>)
tensor(3802.9412, device='cuda:0', grad_fn=<AddBackward0>)
OPT step Epoch-2 Batch-14 Step-122 TIME ELAPSED = 619.298526763916
Params after OPT step  params tensor([ 0.0231, -0.0420, -0.0113,  ..., -0.0614, -0.0848,  0.8135],
       device='cuda:0')
Params after OPT step  conv_layer1.kernel tensor([[ 1.5034,  0.1169,  0.8287,  1.5025,  0.6959],
        [ 0.7458, -0.6478,  1.0730,  1.6057, -0.1893],
        [-0.2320,  0.9042,  0.6533, -0.5316,  1.2535],
        [-0.4859,  0.5855,  1.5877,  1.5304,  0.9413],
        [ 1.7107,  1.1779, -0.5233,  0.7151,  0.7810]], device='cuda:0')
Params after OPT step  conv_layer2.kernel tensor([[-0.2946, -0.9474,  0.1743],
        [ 2.5350,  0.9983, -0.9450],
        [-0.6331, -0.7369,  1.2415]], device='cuda:0')
Starting Accuracy Calculation ....
Getting Training predictions...
Getting Test predictions...

Training Accuracy : 
	word accuracy =  [0.0625, 0.046875, 0.171875, 0.109375, 0.0625, 0.078125, 0.109375, 0.078125, 0.21875, 0.1875, 0.34375, 0.203125, 0.265625, 0.296875, 0.265625, 0.375, 0.265625, 0.28125, 0.265625, 0.25, 0.265625, 0.34375, 0.328125, 0.40625, 0.3125, 0.3125, 0.390625, 0.234375, 0.515625, 0.5, 0.46875, 0.53125, 0.5625, 0.421875, 0.46875, 0.53125, 0.28125, 0.515625, 0.390625, 0.453125, 0.515625, 0.390625, 0.390625, 0.484375, 0.515625, 0.375, 0.671875, 0.46875, 0.546875, 0.65625, 0.4375, 0.359375, 0.5, 0.5217391304347826, 0.5625, 0.5625, 0.625, 0.59375, 0.578125, 0.609375, 0.453125, 0.625, 0.53125, 0.546875, 0.4375, 0.390625, 0.46875, 0.40625, 0.734375, 0.578125, 0.609375, 0.5, 0.625, 0.546875, 0.34375, 0.5625, 0.5625, 0.75, 0.421875, 0.421875, 0.6875, 0.53125, 0.609375, 0.59375, 0.46875, 0.625, 0.703125, 0.578125, 0.6875, 0.296875, 0.65625, 0.5625, 0.65625, 0.546875, 0.625, 0.4375, 0.6875, 0.390625, 0.65625, 0.703125, 0.53125, 0.671875, 0.5625, 0.65625, 0.5625, 0.671875, 0.71875, 0.717391304347826, 0.484375, 0.546875, 0.5, 0.578125, 0.515625, 0.5625, 0.3125, 0.6875, 0.40625, 0.671875, 0.421875, 0.5625, 0.671875, 0.59375, 0.515625]
	letter accuracy =  [0.5727482678983834, 0.6561181434599156, 0.6634799235181644, 0.6823770491803278, 0.6693711967545639, 0.6461232604373758, 0.7075664621676891, 0.6403846153846153, 0.7103594080338267, 0.7434343434343434, 0.7626050420168067, 0.7698744769874477, 0.746938775510204, 0.7962577962577962, 0.7665289256198347, 0.8366890380313199, 0.7676950998185118, 0.8139059304703476, 0.7547528517110266, 0.7837301587301587, 0.7715430861723447, 0.852589641434263, 0.8004246284501062, 0.827433628318584, 0.8360323886639676, 0.8114406779661016, 0.8380566801619433, 0.802020202020202, 0.8961038961038961, 0.8625, 0.8788501026694046, 0.8947368421052632, 0.8946135831381733, 0.8236472945891784, 0.8738938053097345, 0.8865546218487395, 0.8003952569169961, 0.8903803131991052, 0.8565573770491803, 0.8687258687258688, 0.8726114649681529, 0.8349705304518664, 0.8519269776876268, 0.879492600422833, 0.8918367346938776, 0.8283433133732535, 0.9419354838709677, 0.8777120315581854, 0.9130434782608695, 0.933649289099526, 0.835030549898167, 0.8597285067873304, 0.8877755511022044, 0.9075144508670521, 0.8956521739130435, 0.9092741935483871, 0.9265658747300216, 0.9080717488789237, 0.8923076923076924, 0.9307535641547862, 0.9016736401673641, 0.9239766081871345, 0.8725490196078431, 0.8992094861660079, 0.8848484848484849, 0.882466281310212, 0.8978805394990366, 0.8662900188323918, 0.9364303178484108, 0.9251012145748988, 0.9225941422594143, 0.8926315789473684, 0.911522633744856, 0.9026915113871635, 0.8260869565217391, 0.9019607843137255, 0.895397489539749, 0.9518599562363238, 0.8846960167714885, 0.8754940711462451, 0.9495145631067962, 0.8897795591182365, 0.9166666666666666, 0.9189765458422174, 0.8703703703703703, 0.9252747252747253, 0.9427966101694916, 0.918141592920354, 0.927063339731286, 0.8257425742574257, 0.9359823399558499, 0.9138276553106213, 0.9348659003831418, 0.9083503054989817, 0.9291666666666667, 0.8738049713193117, 0.9258474576271186, 0.8448979591836735, 0.9288888888888889, 0.9503968253968254, 0.8858350951374208, 0.9473684210526315, 0.8888888888888888, 0.9206963249516441, 0.9067460317460317, 0.9368191721132898, 0.9458874458874459, 0.936046511627907, 0.8708414872798435, 0.8920704845814978, 0.9089219330855018, 0.8997722095671982, 0.9046653144016227, 0.9148073022312373, 0.8574423480083857, 0.9451219512195121, 0.8775100401606426, 0.9398797595190381, 0.8721973094170403, 0.9024390243902439, 0.9424778761061947, 0.9126637554585153, 0.8985801217038539]
Test Accuracy : 
	word accuracy =  [0.03125, 0.078125, 0.0625, 0.046875, 0.09375, 0.109375, 0.078125, 0.015625, 0.15625, 0.25, 0.0625, 0.109375, 0.09375, 0.109375, 0.140625, 0.109375, 0.15625, 0.15625, 0.109375, 0.109375, 0.125, 0.109375, 0.109375, 0.1875, 0.015625, 0.171875, 0.125, 0.046875, 0.109375, 0.203125, 0.125, 0.046875, 0.109375, 0.125, 0.09375, 0.0625, 0.203125, 0.125, 0.125, 0.03125, 0.078125, 0.046875, 0.109375, 0.140625, 0.203125, 0.046875, 0.125, 0.203125, 0.109375, 0.125, 0.171875, 0.0625, 0.15625, 0.0625, 0.109375, 0.1875, 0.078125, 0.09375, 0.140625, 0.15625, 0.171875, 0.109375, 0.0625, 0.1875, 0.09375, 0.078125, 0.046875, 0.140625, 0.078125, 0.125, 0.15625, 0.125, 0.140625, 0.109375, 0.25, 0.109375, 0.265625, 0.125, 0.21875, 0.09375, 0.140625, 0.09375, 0.140625, 0.09375, 0.078125, 0.125, 0.109375, 0.109375, 0.046875, 0.078125, 0.125, 0.078125, 0.21875, 0.125, 0.109375, 0.109375, 0.140625, 0.296875, 0.09375, 0.140625, 0.125, 0.15625, 0.078125, 0.078125, 0.109375, 0.078125, 0.09375, 0.078125, 0.109375, 0.109375, 0.078125, 0.171875, 0.15625, 0.09375, 0.171875, 0.15625, 0.09375, 0.0625, 0.140625, 0.09375, 0.1875, 0.09375, 0.09375]
	letter accuracy =  [0.5225933202357563, 0.5975855130784709, 0.6021739130434782, 0.6069868995633187, 0.6136363636363636, 0.6419047619047619, 0.6186612576064908, 0.509469696969697, 0.6291390728476821, 0.6818181818181818, 0.5769230769230769, 0.6633663366336634, 0.6296296296296297, 0.6621621621621622, 0.6632653061224489, 0.5903890160183066, 0.7069672131147541, 0.6606741573033708, 0.6839186691312384, 0.5849462365591398, 0.6118811881188119, 0.6307977736549165, 0.6998050682261209, 0.5685071574642127, 0.6119096509240246, 0.6455696202531646, 0.6591760299625468, 0.656140350877193, 0.579295154185022, 0.685546875, 0.6175824175824176, 0.5852631578947368, 0.6115537848605578, 0.6288032454361054, 0.6382978723404256, 0.5983935742971888, 0.6888888888888889, 0.6276391554702495, 0.6869747899159664, 0.6131078224101479, 0.6404715127701375, 0.6098484848484849, 0.6515151515151515, 0.6234042553191489, 0.7079439252336449, 0.5591182364729459, 0.6565656565656566, 0.6174757281553398, 0.5859213250517599, 0.6381156316916489, 0.6815415821501014, 0.6142857142857143, 0.6950959488272921, 0.5901639344262295, 0.646484375, 0.6699029126213593, 0.5872093023255814, 0.6123260437375746, 0.6365591397849463, 0.6374501992031872, 0.6829745596868885, 0.6839080459770115, 0.5928411633109619, 0.6687370600414079, 0.6345381526104418, 0.6574257425742575, 0.5977777777777777, 0.700836820083682, 0.6216730038022814, 0.6034858387799564, 0.6521739130434783, 0.6571428571428571, 0.6509240246406571, 0.6647940074906367, 0.6829268292682927, 0.6158415841584158, 0.7034313725490197, 0.6576200417536534, 0.6835164835164835, 0.6262626262626263, 0.6444007858546169, 0.6378269617706237, 0.6518046709129511, 0.5668103448275862, 0.5588235294117647, 0.6421663442940039, 0.6275720164609053, 0.6952191235059761, 0.5803921568627451, 0.679920477137177, 0.6189473684210526, 0.6147368421052631, 0.6936936936936937, 0.6032388663967612, 0.616600790513834, 0.5641547861507128, 0.6391304347826087, 0.7586206896551724, 0.5840163934426229, 0.650103519668737, 0.6335282651072125, 0.6263982102908278, 0.5610236220472441, 0.6329113924050633, 0.645224171539961, 0.6203904555314533, 0.6680584551148225, 0.6333333333333333, 0.5936883629191322, 0.6059322033898306, 0.6218487394957983, 0.5861297539149888, 0.6239669421487604, 0.611439842209073, 0.6514522821576764, 0.6607142857142857, 0.6647398843930635, 0.5755693581780539, 0.6889352818371608, 0.6244725738396625, 0.6457399103139013, 0.6282306163021869, 0.5708884688090737]